# 操作系统

## 一 进程通信方法

每个进程各自有不同的用户地址空间,任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核,在内核中开辟一块缓冲区,进程A把数据从用户空间拷到内核缓冲区,进程B再从内核缓冲区把数据读走,内核提供的这种机制称为进程间通信。

### 1 匿名管道通信

------

匿名管道( pipe )：管道是一种**半双工**的通信方式，**数据只能单向流动**，而且只能在具**有亲缘关系的进程间**使用。进程的亲缘关系通常是指**父子进程关系**。

```vala
// 需要的头文件
#include <unistd.h>

// 通过pipe()函数来创建匿名管道
// 返回值：成功返回0，失败返回-1
// fd参数返回两个文件描述符
// fd[0]指向管道的读端，fd[1]指向管道的写端
// fd[1]的输出是fd[0]的输入。
int pipe (int fd[2]);
```

通过匿名管道实现进程间通信的步骤如下：

- 父进程创建管道，得到两个⽂件描述符指向管道的两端
- 父进程fork出子进程，⼦进程也有两个⽂件描述符指向同⼀管道。
- 父进程关闭fd[0],子进程关闭fd[1]，即⽗进程关闭管道读端,⼦进程关闭管道写端（因为管道只支持单向通信）。⽗进程可以往管道⾥写,⼦进程可以从管道⾥读,管道是⽤环形队列实现的,数据从写端流⼊从读端流出,这样就实现了进程间通信。



### 2 有名管道通信

------

有名管道 (named pipe) ： 有名管道也是**半双工**的通信方式，但是它**允许无亲缘关系进程间的通信**。



### 3 [消息队列](https://cloud.tencent.com/product/cmq?from=10680)通信

------

消息队列( message queue ) ： 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。



### 4 信号量通信

------

信号量( semophore ) ： 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。

### 5 信号

------

信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。

### 6 共享内存通信

------

共享内存( shared memory ) ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。

### 7 套接字通信



## 二 孤儿进程僵尸进城

## 三 死锁条件，如何避免

## 四 哈希的实现有哪几种，如何取hashcode，冲突检测几种方法





# 计算机网络



## 一、time-wait的作用

## 二、http能不能一次连接多次请求，不等后端返回

## 三、TCP 有哪些状态

## 四、建立一个 socket 连接要经过哪些步骤

## 五  TCP 拥塞控制（快速恢复、快速重传）

## 六 https 握手，为什么需要 非对称加密 和 对称加密

## 七 io多路复用，epoll和select的区别

## 八   cookie session







# docker







# k8s



## 一、简述Kubernetes创建一个Pod的主要流程？

Kubernetes中创建一个Pod涉及多个组件之间联动，主要流程如下：

- 1、客户端提交Pod的配置信息（可以是yaml文件定义的信息）到kube-apiserver。
- 2、Apiserver收到指令后，通知给controller-manager创建一个资源对象。
- 3、Controller-manager通过api-server将pod的配置信息存储到ETCD数据中心中。
- 4、Kube-scheduler检测到pod信息会开始调度预选，会先过滤掉不符合Pod资源配置要求的节点，然后开始调度调优，主要是挑选出更适合运行pod的节点，然后将pod的资源配置单发送到node节点上的kubelet组件上。
- 5、Kubelet根据scheduler发来的资源配置单运行pod，运行成功后，将pod的运行信息返回给scheduler，scheduler将返回的pod运行状况的信息存储到etcd数据中心。



## Kubernetes中pod的创建流程

一般我们在创建pod的过程中都是，执行kubectl命令去apply对应的yaml文件，但是在执行这个操作的过程到pod被完成创建，k8s的组件都做了哪些操作呢？下面我们简要说说pod被创建的过程。

![image-20211011193626400](整体面试题.assets/image-20211011193626400.png)

1.用户通过kubectl命名发起请求。

2.apiserver通过对应的kubeconfig进行认证，认证通过后将yaml中的po信息存到etcd。

\3. Controller-Manager通过apiserver的watch接口发现了pod信息的更新，执行该资源所依赖的拓扑结构整合，整合后将对应的信息写到etcd，此时pod已经可以被调度了。

4.Scheduler同样通过apiserver的watch接口更新到pod可以被调度，通过算法给pod分配节点，并将pod和对应节点绑定的信息写到etcd，然后将pod交给kubelet。

5.kubelet收到pod后，调用CNI接口给pod创建pod网络，调用CRI接口去启动容器，调用CSI进行存储卷的挂载。

6.网络，容器，存储创建完成后pod创建完成，等业务进程启动后，pod运行成功。



## 二、k8s 核心组件

### **1.Master节点（默认不参加工作）**

```text
Kubectl：
 客户端命令行工具，作为整个K8s集群的操作入口；

Api Server：
 在K8s架构中承担的是“桥梁”的角色，作为资源操作的唯一入口，它提供了认证、授权、访问控制、API注册和发现等机制。客户端与k8s群集及K8s内部组件的通信，都要通过Api Server这个组件；

Controller-manager：
 负责维护群集的状态，比如故障检测、自动扩展、滚动更新等

Scheduler：
 负责资源的调度，按照预定的调度策略将pod调度到相应的node节点上；
```

> Etcd(可以不在master节点)：担任数据中心的角色，保存了整个群集的状态；

### **2.Node节点**

```text
Kubelet：
 负责维护容器的生命周期，同时也负责Volume和网络的管理，一般运行在所有的节点，是Node节点的代理，当Scheduler确定某个node上运行pod之后，会将pod的具体信息（image，volume）等发送给该节点的kubelet，kubelet根据这些信息创建和运行容器，并向master返回运行状态。（自动修复功能：如果某个节点中的容器宕机，它会尝试重启该容器，若重启无效，则会将该pod杀死，然后重新创建一个容器）；

Kube-proxy：
 Service在逻辑上代表了后端的多个pod。负责为Service提供cluster内部的服务发现和负载均衡（外界通过Service访问pod提供的服务时，Service接收到的请求后就是通过kube-proxy来转发到pod上的）；
 
container-runtime：
 是负责管理运行容器的软件，比如docker
```



Master服务端（主控节点）主要负责管理和控制整个Kubernetes集群，对集群做出全局性决策，相当于整个集群的“大脑”。集群所执行的所有控制命令都由Master服务端接收并处理。Master服务端主要包含如下组件。

● kube-apiserver组件：集群的HTTP REST API接口，**是集群控制的入口。**

● kube-controller-manager组件：**集群中所有资源对象的自动化控制中心。**

● kube-scheduler组件：集群中Pod资源对象的**调度服务**。

Node客户端（工作节点）是Kubernetes集群中的工作节点，Node节点上的工作由Master服务端进行分配，比如当某个Node节点宕机时，Master节点会将其上面的工作转移到其他Node节点上。Node节点主要包含如下组件。

● kubelet组件：**负责管理节点上容器的创建、删除、启停等任务**，**与Master节点进行通信**。

● kube-proxy组件：负责Kubernetes**服务的通信及负载均衡服务。**

● container组件：负责容器的基础管理服务，接收kubelet组件的指令。



## 三、etcd





## 四、controller 原理 和 operator 



## 五、网络互通

### 1. 同 node pod to pod

### 2. 不同 node pod to pod



## 六 怎么扩展 kubernetes scheduler, 让它能 handle 大规模的节点调度（蚂蚁金服面试题）



## 七 k8s 的 exec 是怎么实现的?（蚂蚁金服面试题）



## 八 有没有写过 k8s 的 Operator 或 Controller？（蚂蚁金服面试题）



## 九 对 Kubernetes 了解怎么样，看过源码吗？apiserver、scheduler、controller-manager

​            

## 十 谈一谈你对微服务架构的理解

微服务架构（MSA）的基础是将单个应用程序开发为一组小型独立服务，这些独立服务在自己的进程中运行，独立开发和部署。

**这些服务使用轻量级 API 通过明确定义的接口进行通信**。这些服务是围绕业务功能构建的，每项服务执行一项功能。由于它们是独立运行的，因此可以针对各项服务进行更新、部署和扩展，以满足对应用程序特定功能的需求。

### 微服务特性

- 自主性

**可以对微服务架构中的每个组件服务进行开发、部署、运营和扩展，而不影响其他服务的功能**。这些服务不需要与其他服务共享任何代码或实施。**各个组件之间的任何通信都是通过明确定义的 API 进行的。**

- 专用性

**每项服务都是针对一组功能而设计的，并专注于解决特定的问题**。如果开发人员逐渐将更多代码增加到一项服务中并且这项服务变得复杂，那么可以将其拆分成多项更小的服务。

### 微服务的优势

- 敏捷性

微服务促进若干小型独立团队形成一个组织，这些团队负责自己的服务。各团队在小型且易于理解的环境中行事，并且可以更独立、更快速地工作。**这缩短了开发周期时间**。您可以从组织的总吞吐量中显著获益。

- 扩展性

通过微服务，您可以独立扩展各项服务以满足其支持的应用程序功能的需求。这使团队能够适当调整基础设施需求，准确衡量功能成本，并在服务需求激增时保持可用性。

- 轻松部署

**微服务支持持续集成和持续交付，可以轻松尝试新想法，并可以在无法正常运行时回滚**。由于故障成本较低，因此可以大胆试验，更轻松地更新代码，并缩短新功能的上市时间。

- 代码可重用性

将软件划分为小型且明确定义的模块，让团队可以将功能用于多种目的。专为某项功能编写的服务可以用作另一项功能的构建块。这样应用程序就可以自行引导，因为开发人员可以创建新功能，而无需从头开始编写代码。

- 较好的弹性设计

服务独立性增加了应用程序应对故障的弹性。**在整体式架构中，如果一个组件出现故障，可能导致整个应用程序无法运行。通过微服务，应用程序可以通过降低功能而不导致整个应用程序崩溃来处理总体服务故障。**





## 十一 Informer 是怎么实现的，有什么作用？



1. **Reflector 通过ListWatcher 同步apiserver 数据（只启动时搞一次），并watch apiserver ，将event 加入到 Queue 中**
2. **controller 从 Queue中获取event，更新存储，并触发Processor 业务层注册的 ResourceEventHandler**

 

## 十二 Kubernetes 的所有资源约定了版本号, 为什么要这么做?

为了在兼容旧版本的同时不断升级新的 API，Kubernetes 支持多种 API 版本，不同的 API 版本代表其处于不同的稳定性阶段，低稳定性的 API 版本在后续的产品升级中可能成为高稳定性的版本。

为了简化删除字段或者重构资源表示等工作，Kubernetes 支持多个 API 版本， 每一个版本都在不同 API 路径下，例如 `/api/v1` 或 `/apis/rbac.authorization.k8s.io/v1alpha1`。



## 十三  raft算法是那种一致性算法





## 十四 Controller Manager & Controller 工作原理概述

Controller Manager 的作用简而言之：**保证集群中各种资源的实际状态（status）和用户定义的期望状态（spec）一致。**

Controller Manager 就是集群内部的管理控制中心

之所以叫 Controller Manager，是因为 Controller Manager 由负责不同资源的多个 Controller 构成，如 Deployment Controller、Node Controller、Namespace Controller、Service Controller 等，这些 Controllers 各自明确分工负责集群内资源的管理。

![image-20200725172912964](https://blog.yingchi.io/posts/2020/7/k8s-cm-informer/image-20200725172912964.png)



## 十五 client-go

Controller Manager 中一个很关键的部分就是 client-go，client-go 在 controller manager 中起到向 controllers 进行事件分发的作用。目前 client-go 已经被单独抽取出来成为一个项目了，除了在 kubernetes 中经常被用到，在 kubernetes 的二次开发过程中会经常用到 client-go，比如可以通过 client-go 开发自定义 controller。



**lient-go 包中一个非常核心的工具就是 informer**，informer 可以让与 kube-apiserver 的交互更加优雅。

informer 主要功能可以概括为两点：

- 资源数据缓存功能，**缓解对 kube-apiserver 的访问压力；**
- 资源事件分发，**触发事先注册好的 ResourceEventHandler；**

Informer 另外一块内容在于提供了事件 handler 机制，并会触发回调，这样 Controller 就可以基于回调处理具体业务逻辑。因为 Informer 通过 List、Watch 机制可以监控到所有资源的所有事件，**因此只要给 Informer 添加ResourceEventHandler 实例的回调函数实例取实现 `OnAdd(obj interface{})`、 `OnUpdate(oldObj, newObj interface{}) ` 和 `OnDelete(obj interface{})` 这三个方法**，就可以处理好资源的创建、更新和删除操作

### client-go 工作机制

![img](https://blog.yingchi.io/posts/2020/7/k8s-cm-informer/client-go-controller-interaction.jpeg)

上图是官方给出的 client-go 与自定义 controller 的实现原理。

#### Reflactor

反射器，具有以下几个功能：

- 采用 List、Watch 机制与 kube-apiserver 交互，List 短连接获取全量数据，Watch 长连接获取增量数据；
- 可以 Watch 任何资源包括 CRD；
- Watch 到的增量 Object 添加到 Delta FIFO 队列，然后 Informer 会从队列里面取数据;

#### Informer

Informer 是 client-go 中较为核心的一个模块，其主要作用包括如下两个方面：

- 同步数据到本地缓存。**Informer 会不断读取 Delta FIFO 队列中的 Object，在触发事件回调之前先更新本地的 store**，如果是新增 Object，如果事件类型是 Added（添加对象），那么 Informer 会通过 Indexer 的库把这个增量里的 API 对象保存到本地的缓存中，并为它创建索引。**之后通过 Lister 对资源进行 List / Get 操作时会直接读取本地的 store 缓存，通过这种方式避免对 kube-apiserver 的大量不必要请求，缓解其访问压力；**
- **根据对应的事件类型，触发事先注册好的 ResourceEventHandler**。client-go 的 informer 模块启动时会创建一个 shardProcessor，各种 controller（如 Deployment Controller、自定义 Controller…）的事件 handler 注册到 informer 的时候会转换为一个 processorListener 实例，然后 processorListener 会被 append 到 shardProcessor 的 Listeners 切片中，shardProcessor 会管理这些 listeners。

processorListener 的重要作用就是当事件到来时触发对应的处理方法，因此不停地从 nextCh 中拿到事件并执行对应的 handler。`sharedProcessor` 的职责便是管理所有的 Handler 以及分发事件，而真正做分发工作的是 `distribute` 方法。

梳理一下这中间的过程：

1. Controller 将 Handler 注册给 Informer；
2. Informer 通过 `sharedProcessor` 维护了所有转换为 processorListener 的 Handler；
3. Informer 收到事件时，通过 `sharedProcessor.distribute` 将事件分发下去；
4. Controller 被触发对应的 Handler 来处理自己的逻辑。

![img](https://blog.yingchi.io/posts/2020/7/k8s-cm-informer/2019-12-15-eventdistribute.jpg)

Reflactor 启动后会执行一个 processLoop 死循环，Loop 中不停地将 Delta FIFO 队列中的事件 Pop 出来，Pop 时会取出该资源的所有事件，并交给 `sharedIndexInformer` 的 `HandleDeltas` 方法（创建 `controller` 时赋值给了 `config.Process`，传递到 Pop 参数的处理函数中 `Pop(PopProcessFunc(c.config.Process))`），`HandleDeltas` 调用了 `processor.distribute` 完成事件的分发。

在注册的 ResourceEventHandler 回调函数中，只是做了一些很简单的过滤，然后将关心变更的 Object 放到 workqueue 里面。之后 Controller 从 workqueue 里面取出 Object，启动一个 worker 来执行自己的业务逻辑，通常是对比资源的当前运行状态与期望状态，做出相应的处理，实现运行状态向期望状态的收敛。

注意，在 worker 中就可以使用 lister 来获取 resource，这个时候不需要频繁的访问 kube-apiserver 了，对于资源的 List / Get 会直接访问 informer 本地 store 缓存，apiserver 中资源的的变更都会反映到这个缓存之中。同时，LocalStore 会周期性地把所有的 Pod 信息重新放到 DeltaFIFO 中。



## 十六 网络 CNI Flannel



### 从网络模型到 CNI

从底层网络来看，kubernetes 的网络通信可以分为三层去看待：

- Pod 内部容器通信；
- 同主机 Pod 间容器通信；
- 跨主机 Pod 间容器通信；

对于前两点，其网络通信原理其实不难理解。

1. 对于 Pod 内部容器通信，由于 Pod 内部的容器处于同一个 **Network Namespace** 下（**通过 Pause 容器实现**），即共享同一网卡，因此可以直接通信。
2. 对于同主机 Pod 间容器通信，**Docker 会在每个主机上创建一个 Docker0 网桥**，**主机上面所有 Pod 内的容器全部接到网桥上**，因此可以互通。

而对于第三点，跨主机 Pod 间容器通信，Docker 并没有给出很好的解决方案，而对于 Kubernetes 而言，跨主机 Pod 间容器通信是非常重要的一项工作，但是有意思的是，Kubernetes 并没有自己去解决这个问题，而是专注于容器编排问题，对于跨主机的容器通信则是交给了第三方实现，这就是 CNI 机制。

CNI，它的全称是 Container Network Interface，即容器网络的 API 接口。kubernetes 网络的发展方向是希望通过插件的方式来集成不同的网络方案， CNI 就是这一努力的结果。CNI 只专注解决容器网络连接和容器销毁时的资源释放，提供一套框架，所以 CNI 可以支持大量不同的网络模式，并且容易实现。平时比较常用的 CNI 实现有 Flannel、Calico、Weave 等。

CNI 插件通常有三种实现模式：

- Overlay：靠隧道打通，不依赖底层网络；
- 路由：靠路由打通，部分依赖底层网络；
- Underlay：靠底层网络打通，强依赖底层网络；

在选择 CNI 插件时是要根据自己实际的需求进行考量，比如考虑 NetworkPolicy 是否要支持 Pod 网络间的访问策略，可以考虑 Calico、Weave；Pod 的创建速度，Overlay 或路由模式的 CNI 插件在创建 Pod 时比较快，Underlay 较慢；网络性能，Overlay 性能相对较差，Underlay 及路由模式相对较快。

## Flannel 工作原理

CNI 中经常见到的解决方案是 Flannel，由CoreOS推出，Flannel 采用的便是上面讲到的 Overlay 网络模式。



**内容太多了，单独抽出了一篇文章看**





# go



## 1. goroutine 里面 panic 了会怎么样



在Go语言中，我们通常会用到panic和recover来抛出错误和捕获错误，这一对操作在单协程环境下我们正常用就好了，并不会踩到什么坑。但是在多协程并发环境下，我们常常会碰到以下两个问题。假设我们现在有2个协程，我们叫它们协程A和B好了：

- 如果协程A发生了panic，协程B是否会因为协程A的panic而挂掉？
- 如果协程A发生了panic，协程B是否能用recover捕获到协程A的panic？

答案分别是：会、不能。



**哪个协程发生了panic，我们就需要在哪个协程recover**



## 2. Go 中 defer 机制，可以返回数据吗

### defer概述

`defer` 是`golang` 中独有的流程控制语句，用于延迟指定语句的运行时机，只能运行于函数的内部，且当他所属函数运行完之后它才会被调用。例如：

```
func deferTest(){
    defer fmt.Println("HelloDefer")
    fmt.Println("HelloWorld")
}
```

它会先打印出`HelloWorld` ，然后再打印出`HelloDefer` 。

一个函数中如果有多个`defer` ，运行顺序和函数中的调用顺序相反，因为它们都是被写在了栈中：

```go
func deferTest(){
    defer fmt.Println("HelloDefer1")
    defer fmt.Println("HelloDefer2")
    fmt.Println("HelloWorld")
}
```

运行结果：

```go
fmt.Println("HelloDefer2")
fmt.Println("HelloDefer1")
fmt.Println("HelloWorld")
```

### defer和return

在包含有`return` 语句的函数中**，`defer` 的运行顺序位于`return` 之后，但是`defer` 所运行的代码片段会生效**：

```go
func main(){
    fmt.Println(deferReturn)
}
func deferReturn() int{
    i := 1
    defer func(){
        fmt.Println("Defer")
        i += 1
    }()
    return func()int{
        fmt.Println("Return")
        return i
    }()
}
```

运行结果：

```
Return
Defer
1
```

这里很明显就能看到`defer` 是在`return` 之后运行的！但是有一个问题是`defer` 里执行了语句`i +=1` ，按照这个逻辑的话返回的`i` 值应该是`2` 而不是`1` 。这个问题是由于`return` 的运行机制导致的：`return` 在返回一个对象时，如果返回类型不是指针或者引用类型，那么`return` 返回的就不会是这个对象本身，而是这个对象的副本。

我们可以验证这一个观点：

```go
func main(){
    ...
    fmt.Println("main:    ", x, &x)
}
func deferReturn() int{
    ...
    defer ...{
        fmt.Println("Defer:    ", i, &i)
        ...
    }()
    return ...{
        fmt.Println("Return:    ", i, &i)
        ...
    }()
}
```

程序的输出为：

```
Return:     1 0xc042008238
Defer:     1 0xc042008238
main:     1 0xc042008230  //main函数中的i的地址和deferReturn()中的i的地址是不一样的
```

如果把函数的返回值改成指针类型，这时候的main函数中的返回值就会和函数体内的一致：

```
func main(){
    x := deferReturn()
    fmt.Println("main:    ", x, *x)
}
func deferReturn()*int{
    i := 1
    p := &i
    defer func() {
        *p += 1
        fmt.Println("defer:    ", p, *p)
    }()
    return func() *int{
        fmt.Println("Return:    ", p, *p)
        return p
    }()
}
```

结果：

```
Return:     0xc0420361d0 1
defer:     0xc0420361d0 2
main:     0xc0420361d0 2
```

### defer与return的执行顺序

首先看个例子：

```go
package main

import (
    "fmt"
)

func main() {
    ret := test()
    fmt.Println("test return:", ret)
}

func test() ( int) {
    var i int

    defer func() {
        i++        //defer里面对i增1
        fmt.Println("test defer, i = ", i)
    }()

    return i
}
```

执行结果为：

```subunit
test defer, i =  1
test return: 0
```

test函数的返回值为0，defer里面的i++操作好像对返回值并没有什么影响。
这是否表示“return i”执行结束以后才执行defer呢？
非也！再看下面的例子：

```go
package main

import (
    "fmt"
)

func main() {
    ret := test()
    fmt.Println("test return:", ret)
}

//返回值改为命名返回值
func test() (i int) {
    //var i int

    defer func() {
        i++
        fmt.Println("test defer, i = ", i)
    }()

    return i
}
```

执行结果为：

```subunit
test defer, i =  1
test return: 1
```

这次test函数的返回值变成了1，defer里面的“i++"修改了返回值。所以defer的执行时机应该是return之后，且返回值返回给调用方之前。
至于第一个例子中test函数返回值不是1的原因，还涉及到函数匿名返回值与命名返回值的差异，以后再单独分析。

#### 结论

1. defer的执行顺序为：后defer的先执行。
2. :heavy_check_mark: **defer的执行顺序在return之后，但是在返回值返回给调用方之前，所以使用defer可以达到修改返回值的目的**。





## 3. go里面goroutine创建数量有限制吗？



- 有，P本地队列有数量限制，不允许超过 256 个
- 过多会占用大量的CPU和内存，并且会导致主进程奔溃





## 4. select可以用于什么

**在多个通道上进行读或写操作，让函数可以处理多个事情，但1次只处理1个。以下特性也都必须熟记于心：**

1. 每次执行select，都会只执行其中1个case或者执行default语句。
2. 当没有case或者default可以执行时，select则阻塞，等待直到有1个case可以执行。
3. 当有多个case可以执行时，则随机选择1个case执行。
4. `case`后面跟的必须是读或者写通道的操作，否则编译出错。



## 5. go map实现

#### sync.Map实现原理，适用的场景

这里会用另外一篇文章详细介绍



## 6. Go GC算法，三色标记法描述



## 7. Go内存模型(tcmalloc)



## 8. Go channel

- 在不能更改channel状态的情况下，没有简单普遍的方式来检查channel是否已经关闭了
- 关闭已经关闭的channel会导致panic，所以在closer(关闭者)不知道channel是否已经关闭的情况下去关闭channel是很危险的
- 发送值到已经关闭的channel会导致panic，所以如果sender(发送者)在不知道channel是否已经关闭的情况下去向channel发送值是很危险的



## 9. 判断 channel 是否已经被关闭

1. **从channel读取数据 ** (OK法) 

-  **第二个字段为true时，channel可能没关闭，也可能已经关闭，不能证明什么**
-  **第二个字段为false时，可以证明channel中已没有残留数据且已关闭**
-  **若channel已关闭 并且通道为空，那么从该channel中读取数据会直接返回，且是默认值，所以一定要判断第二个字段**

2. `for-range`是使用频率很高的结构，常用它来遍历数据，**`range`能够感知channel的关闭，当channel被发送数据的协程关闭时，range就会结束**，接着退出for循环。



## 10 关闭后的通道有以下特点：

1. 对一个关闭的通道再发送值就会导致panic。
2. **对一个关闭的通道进行接收会一直获取值直到通道为空**。
3. **对一个关闭的并且没有值的通道执行接收操作会得到对应类型的零值。**
4. 关闭一个已经关闭的通道会导致panic。



## 11 go select介绍

A. select机制用来处理异步IO问题

B. select机制最大的一条限制就是每个case语句里必须是一个IO操作

### select 随机性

switch 里的 case 是顺序执行的，但在 select 里却不是。

### 避免造成死锁

没有写 default 分支，select 就会阻塞，直到有某个 case 可以命中，而如果一直没有命中，select 就会抛出 `deadlock` 的错误

**总结**

1. select **只能用于 channel 的操作(写入/读出)**，而 switch 则更通用一些；
2. select 的 **case 是随机的**，而 switch 里的 case 是顺序执行；
3. select 要注意避免出现死锁，同时也可以自行实现超时机制；
4. select 里没有类似 switch 里的 fallthrough 的用法；
5. select 不能像 switch 一样接函数或其他表达式。



## 12 map如何顺序读取

- 需要另外维护一个数据结构来保持有序的key，然后根据有序key来遍历map。



## 13 slice和array的区别

array是固定长度的数组，使用前必须确定数组长度

**array特点：**

- go的数组是**值类型**，也就是说一个数组赋值给另一个数组，那么实际上就是真个数组拷贝了一份，需要申请额外的内存空间
- 如果go中的数组做为函数的参数，那么**实际传递的参数是一份数组的拷贝**，而不是数组的指针
- array的**长度也是Type的一部分**，这样就说明[10]int和[20]int是不一样的

**slice特点：**

- slice是一个**引用类型**，是一个动态的指向数组切片的指针
- slice是一个不定长的，总是指向底层的数组array的数据结构

**区别：**

- 声明时：array需要声明长度或者...
- 做为函数参数时：**array传递的是数组的副本，slice传递的是指针**



## 14 channel 状态

**channel有三种状态：**

1. nil，未初始化的状态，只进行了声明，或者手动赋值为nil
2. active，正常的channel，可读可写
3. closed，已关闭

**channel可进行三种操作：**

1. 读
2. 写
3. 关闭

**这三种操作和状态可以组合出九种情况：**

| 操作               | nil的channel | 正常channel | 已关闭channel |
| ------------------ | ------------ | ----------- | ------------- |
| <-ch   (读)        | 阻塞         | 成功或阻塞  | 读到零值      |
| ch<-   (写)        | 阻塞         | 成功或阻塞  | panic         |
| close(ch)   (关闭) | panic        | 成功        | panic         |



## 15 在并发状态下map如何保证线程安全

go的map并发访问是不安全的，会出现未定义行为，导致程序退出。

go1.6之前，内置的map类型是部分goroutine安全的，并发的读没有问题，并发的写可能有问题。go1.6之后，并发的读写map会报错。

> 对比一下Java的`ConcurrentHashMap`的实现，在map的数据非常大的情况下，一把锁会导致大并发的客户端争抢一把锁，Java的解决方案是`shard`，内部使用多个锁，每个区间共享一把锁，这样减少了数据共享一把锁的性能影响

go1.9之前，一般情况下通过`sync.RWMutex`实现对map的并发访问控制，或者单独使用锁都可以。

go1.9之后，实现了`sync.Map`，类似于Java的`ConcurrentHashMap`。

`sync.Map`的实现有几个优化点：

1. 空间换时间。通过冗余的两个数据结构（read，dirty），实现加锁对性能的影响
2. 使用只读数据（read），避免读写冲突
3. 动态调整，miss次数多了之后，将dirty数据提升为read
4. double-checking
5. 延迟删除。删除一个键值只是打标记，只有在提升dirty的时候才清理删除的数据
6. 优先从read读取、更新、删除，因为对read的读取不需要锁



## 16 聊聊你对gc的理解

#### 内存管理

go实现的内存管理简单的说就是维护一块大的全局内存，每个线程（go中为P）维护一块小的私有内存，私有内存不足再从全局申请。

- go程序启动时申请一块大内存，并划分成spans、bitmap、arena区域
- arean区域按页划分成一个个小块
- span管理一个或多个页
- mcentral管理多个span供线程申请使用
- mcache作为线程私有资源，资源来源于mcentral

更多说明参阅引用说明[1](#fn-1)

#### 垃圾回收

常见的垃圾回收算法：

- 引用计数：对每个对象维护一个引用计数，当引用该对象的对象被销毁时，引用计数减一，当引用计数为0时回收该对象。
  - 优点：对象可以很快地被回收，不会出现内存耗尽或达到某个阈值时才回收。
  - 缺点：不能很好的处理循环引用，而且实时的维护引用计数，也有一定的代价。
  - 代表语言：Python、PHP、Swift
- 标记-清除：从根变量遍历所有引用的对象，引用对象标记为”被引用“，没有被标记的进行回收。
  - 优点：解决了引用计数的缺点
  - 缺点：需要STW（Stop The World），就是停掉所有的goroutine，专心做垃圾回收，待垃圾回收结束后再恢复goroutine，这回导致程序短时间的暂停。
  - 代表语言：Go（三色标记法）
- 分代收集：按照对象生命周期的长短划分不同的代空间，生命周期长的放入老年代，而短的放入新生代，不同代有不同的回收算法和回收频率。
  - 优点：回收性能好
  - 缺点：回收算法复杂
  - 代表语言：Java

##### Go垃圾回收的三色标记法

三色标记法只是为了描述方便抽象出来的一种说法，实际上对象并没有颜色之分。这里的三色对应了垃圾回收过程中对象的三种状态：

- 灰色：对象还在标记队列中等待
- 黑色：对象已被标记，gcmarkBits对应的位为1（对象不会在本次GC中被清理）
- 白色：对象未被标记，gcmarkBits对应的位为0（对象将会在本次GC中被清理）

#### 垃圾回收优化[2](#fn-2)

##### 写屏障（Write Barrier）

前面说过STW目的是防止GC扫描时内存变化而停掉goroutine，而写屏障就是让goroutine与GC同时运行的手段。虽然写屏障不能完全消除STW，但是可以大大减少STW的时间。

写屏障类似一种开关，在GC的特定时机开启，开启后指针传递时会把指针标记，即本轮不回收，下次GC时再确定。

GC过程中新分配的内存会被立即标记，用的并不是写屏障技术，也即GC过程中分配的内存不会在本轮GC中回收。

##### 辅助GC（Mutator Assist）

为了防止内存分配过快，在GC执行过程中，如果goroutine需要分配内存，那么这个goroutine会参与一部分GC的工作，即帮助GC做一部分的工作，这个机制叫做Mutator Assist。

#### 垃圾回收触发时机[3](#fn-3)

##### 内存分配量达到阈值出发GC

每次内存分配时都会检查当前内存分配量是否已达到阈值，如果达到阈值则立即启动GC。

```
阈值 = 上次GC内存分配量 * 内存增长率
复制代码
```

内存增长率由环境变量`GOGC`控制，默认为100，即每当内存扩大一倍时启动GC。

##### 定期触发GC

默认情况下，最长2分钟触发一次GC，这个间隔在`src/runtime/proc.go:forcegcperiod`变量中被声明：

```go
// forcegcperiod is the maximum time in nanoseconds between garbage
// collections. If we go this long without a garbage collection, one
// is forced to run.
//
// This is a variable for testing purposes. It normally doesn't change.
var forcegcperiod int64 = 2 * 60 * 1e9
复制代码
```

##### 手动触发

程序代码中也可以使用`runtime.GC()`来手动触发GC，这主要用于GC性能测试和统计。

#### GC性能优化

GC性能与对象数量负相关，对象越多GC性能越差，对程序影响越大。

所以GC性能优化的思路之一就是减少对象分配个数，比如对象复用或使用大对象组合多个小对象等等。

另外，由于*内存逃逸现象*，有些隐式的内存分配也会产生，也有可能成为GC的负担。

> 内存逃逸现象[3.1](#fn-3.1)：变量分配在栈上需要能在编译器确定它的作用域，否则就会被分配到堆上。而堆上动态分配内存比栈上静态分配内存，开销大很多。

go通过`go build -gcflags=m`命令来观察变量逃逸情况[4](#fn-4)

更多逃逸场景：[逃逸场景](https://link.juejin.cn?target=https%3A%2F%2Frainbowmango.gitbook.io%2Fgo%2Fchapter04%2F4.3-escape_analysis%233-tao-yi-chang-jing)

**逃逸分析的作用：**

1. 逃逸分析的好处是为了减少GC的压力，不逃逸的对象分配在栈上，当函数返回时就回收了资源，不需要GC标记清除。
2. 逃逸分析完后可以确定哪些变量可以分配在栈上，栈的分配比堆快，性能好（逃逸的局部变量会分配在堆上，而没有发生逃逸的则由编译器分配到栈上）
3. 同步消除，如果你定义的对象在方法上有同步锁，但在运行时，却只有一个线程在访问，此时逃逸分析后的机器码，会去掉同步锁运行

**逃逸总结**

- 栈上分配内存比在堆中分配内存有更高的效率
- 栈上分配的内存不需要GC处理
- 堆上分配的内存使用完毕会交给GC处理
- 逃逸分析的目的是决定内存分配到堆还是栈
- 逃逸分析在编译阶段完成



## 17 GMP 模型，为什么要有 P？

其实这个面试题本质是想问：”**GMP 模型，为什么不是 G 和 M 直接绑定就完了，还要搞多一个 P 出来，那么麻烦，为的是什么，是要解决什么问题吗**？“



### 解密 Go1.0 源码

```c++
static void
schedule(G *gp)
{
	...
	schedlock();                 // 获取全局锁
	if(gp != nil) {
		...
		switch(gp->status){
		case Grunnable:
		case Gdead:
			// Shouldn't have been running!
			runtime·throw("bad gp->status in sched");
		case Grunning:      // 当前 Goroutine 状态从 Running（正在被调度） 状态修改为 Runnable（可以被调度）状态
			gp->status = Grunnable;
			gput(gp);   // 保存当前 Goroutine 的运行状态等信息
			break;
		}

	gp = nextgandunlock();    // 寻找下一个可运行 Goroutine, 释放全局锁给其他调度使用
	gp->readyonstop = 0;
	gp->status = Grunning;
	m->curg = gp;
	gp->m = m;
	...
	runtime·gogo(&gp->sched, 0);   // 将刚刚所获取到的下一个待执行的 Goroutine 运行起来
}

```

- 调用 `schedlock` 方法来获取全局锁。
- 获取全局锁成功后，将当前 Goroutine 状态从 Running（正在被调度） 状态修改为 Runnable（可以被调度）状态。
- 调用 `gput` 方法来保存当前 Goroutine 的运行状态等信息，以便于后续的使用；
- 调用 `nextgandunlock` 方法来寻找下一个可运行 Goroutine，并且释放全局锁给其他调度使用。
- 获取到下一个待运行的 Goroutine 后，将其的运行状态修改为 Running。
- 调用 `runtime·gogo` 方法，将刚刚所获取到的下一个待执行的 Goroutine 运行起来。



我们可以发现一个比较有趣的点。那就是调度器本身（schedule 方法），在正常流程下，是不会返回的，也就是不会结束主流程。

![G-M模型简图](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2bf2b9ea4008477f80152749420f8c08~tplv-k3u1fbpfcp-watermark.awebp)

他会不断地运行调度流程，GoroutineA 完成了，就开始寻找 GoroutineB，寻找到 B 了，就把已经完成的 A 的调度权交给 B，让 GoroutineB 开始被调度，也就是运行。

当然了，也有被正在阻塞（Blocked）的 G。假设 G 正在做一些系统、网络调用，那么就会导致 G 停滞。这时候 M（系统线程）就会被会重新放内核队列中，等待新的一轮唤醒。



### GM 模型的缺点

当前（代指 Go1.0 的 GM 模型）的 Goroutine 调度器限制了用 Go 编写的并发程序的可扩展性，尤其是高吞吐量服务器和并行计算程序。

实现有如下的问题：

- **存在单一的全局 mutex（Sched.Lock）和集中状态管理**：
  - mutex 需要保护所有与 goroutine 相关的操作（创建、完成、重排等），**导致锁竞争严重。**
- Goroutine 传递的问题：
  - goroutine（G）交接（G.nextg）：工作者线程（M's）之间会经常交接可运行的 goroutine。
  - 上述可能会导致延迟增加和额外的开销。每个 M 必须能够执行任何可运行的 G，特别是刚刚创建 G 的 M。
- 每个 M 都需要做内存缓存（M.mcache）：
  - 会导致资源消耗过大（每个 mcache 可以吸纳到 2M 的内存缓存和其他缓存），数据局部性差。
- 频繁的线程阻塞/解阻塞：
  - 在存在 syscalls 的情况下，**线程经常被阻塞和解阻塞。这增加了很多额外的性能开销。**

## GMP 模型

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a5ecbea79fa243678475e9fae7168365~tplv-k3u1fbpfcp-watermark.awebp)

### 带来什么改变 (面试的时候可以回答这部分)

加了 P 之后会带来什么改变呢？我们再更显式的讲一下。

- **每个 P 有自己的本地队列**，大幅度的减轻了对**全局队列**的直接依赖，**所带来的效果就是锁竞争的减少**。而 GM 模型的性能开销大头就是锁竞争。
- **每个 P 相对的平衡上**，在 GMP 模型中也实现了 **Work Stealing 算法**，如果 P 的本地队列为空，则会从全局队列或其他 P 的本地队列中窃取可运行的 G 来运行，**减少空转，提高了资源利用率**。

### 为什么要有 P

这时候就有小伙伴会疑惑了，如果是想实现本地队列、Work Stealing 算法，那为什么不直接在 M 上加呢，M 也照样可以实现类似的组件。为什么又再加多一个 P 组件？

结合 M（系统线程） 的定位来看，若这么做，有以下问题：

- 一般来讲，M 的数量都会多于 P。**像在 Go 中，M 的数量默认是 10000，P 的默认数量的 CPU 核数**。另外由于 M 的属性，也就是如果存在系统阻塞调用，阻塞了 M，又不够用的情况下，M 会不断增加。
- M 不断增加的话，如果本地队列挂载在 M 上，那就意味着本地队列也会随之增加。这显然是不合理的，**因为本地队列的管理会变得复杂**，且 Work Stealing 性能会大幅度下降。
- M 被系统调用阻塞后，我们是期望把他既有未执行的任务分配给其他继续运行的，而不是一阻塞就导致全部停止。

因此使用 M 是不合理的，那么引入新的组件 P，把本地队列关联到 P 上，就能很好的解决这个问题。





## 18  new make 区别

本质上在于 `make` 函数在初始化时，会初始化 `slice`、`chan`、`map` 类型的内部数据结构，`new` 函数并不会。

例如：在 `map` 类型中，合理的长度（len）和容量（cap）可以提高效率和减少开销。

更进一步的区别：

- ```
  make
  ```

  - 能够创建类型所需的内存空间，返回引用类型的本身。
  - 具有使用范围的局限性，仅支持 `channel`、`map`、`slice` 三种类型。
  - 具有独特的优势，`make` 函数会对三种类型的内部数据结构（长度、容量等）赋值。

- ```
  new
  ```

  - 能够创建并分配类型所需的内存空间，返回指针引用（指向内存的指针）。
  - 可被替代，能够通过字面值快速初始化。



## 19 Goroutine 数量控制在多少合适，会影响 GC 和调度？



## 20 两个 interface 可以比较吗？

空接口在保存不同的值后，可以和其他变量值一样使用`==`进行比较操作。空接口的比较有以下几种特性。

interface 的内部实现包含了 2 个字段，类型 `T` 和 值 `V`

#### 1) 类型不同的空接口间的比较结果不相同

保存有类型不同的值的空接口进行比较时，Go语言会优先比较值的类型。因此类型不同，比较结果也是不相同的，代码如下：

```go
// a保存整型
var a interface{} = 100

// b保存字符串
var b interface{} = "hi"

// 两个空接口不相等
fmt.Println(a == b)
```

代码输出如下：

false

#### 2) 不能比较空接口中的动态值

当接口中保存有动态类型的值时，运行时将触发错误，代码如下：

```go
// c保存包含10的整型切片
var c interface{} = []int{10}

// d保存包含20的整型切片
var d interface{} = []int{20}

// 这里会发生崩溃
fmt.Println(c == d)
```

代码运行到第8行时发生崩溃：

panic: runtime error: comparing uncomparable type []int

这是一个运行时错误，提示 []int 是不可比较的类型。下表中列举出了类型及比较的几种情况。

| 类  型          | 说  明                                                       |
| --------------- | ------------------------------------------------------------ |
| map             | 宕机错误，不可比较                                           |
| 切片（[]T）     | 宕机错误，不可比较                                           |
| 通道（channel） | 可比较，必须由同一个 make 生成，也就是同一个通道才会是 true，否则为 false |
| 数组（[容量]T） | 可比较，编译期知道两个数组是否一致                           |
| 结构体          | 可比较，可以逐个比较结构体的值                               |
| 函数            | 可比较                                                       |



## 21 局部变量在堆空间还是栈空间

根据内存管理（分配和回收）方式的不同，可以将内存分为 **堆内存** 和 **栈内存**。

那么他们有什么区别呢？

**堆内存**：由内存分配器和垃圾收集器负责回收

**栈内存**：由编译器自动进行分配和释放

一个程序运行过程中，也许会有多个栈内存，但肯定只会有一个堆内存。

**每个栈内存都是由线程或者协程独立占有**，因此从栈中分配内存不需要加锁，并且栈内存在函数结束后会自动回收，性能相对堆内存好要高。

而堆内存呢？**由于多个线程或者协程都有可能同时从堆中申请内存**，因此在堆中申请内存需要加锁，避免造成冲突，并且堆内存在函数结束后，需要 GC （垃圾回收）的介入参与，如果有大量的 GC 操作，将会吏程序性能下降得历害。

#### 逃逸

局部变量，虽然在函数里声明定义，但是在函数外还会持续的使用。

对于这类局部变量，显然我们是不希望函数退出后将其销毁的。

那怎么办呢？可以从堆区分配内存空间给这类局部变量。





## 22 什么是协程泄露

**协程泄露是指，在程序运行过程中，有一些协程由于某些原因，无法正常退出。**

协程的运行是需要占用内存和 CPU 时间的，一旦这种协程越来越多，会导致内存无端被浪费，CPU 时间片被占用，程序会越来越卡。

那会导致协程泄露的原因有哪些呢？

其实不用死记硬背，**只要有可能导致程序阻塞的，都有可能会导致协程泄露。**

那么会让程序阻塞的，无外乎：

- 通道阻塞    channel
- 锁阻塞        lock 
- 等待阻塞    waitGroup



### 1. 通道使用不当

通道和协程是 Go 的两大杀器，配合好了是非常香的，但要是没用好，就会造成协程泄露。

下面是常见的一些通道使用不当导致的协程泄露例子

**发送了却没全接收**

```go
func main() {
    for i := 0; i < 4; i++ {
        queryAll()
        fmt.Printf("goroutines: %d\n", runtime.NumGoroutine())
    }
}

func queryAll() int {
    ch := make(chan int)
    for i := 0; i < 3; i++ {
        go func() { ch <- query() }()
        }
    return <-ch
}

func query() int {
    n := rand.Intn(100)
    time.Sleep(time.Duration(n) * time.Millisecond)
    return n
}
```

**没发送却有人在接收**

```go
func main() {
    defer func() {
        fmt.Println("goroutines: ", runtime.NumGoroutine())
    }()

    var ch chan struct{}
    go func() {
        ch <- struct{}{}
    }()

    time.Sleep(time.Second)
}
```

**初始化通道却没分配内存**

```go
func main() {
    defer func() {
        fmt.Println("goroutines: ", runtime.NumGoroutine())
    }()

    var ch chan int
    go func() {
        <-ch
    }()

    time.Sleep(time.Second)
}
```

### 2. 锁使用不当

**加互斥锁后没有解锁**

加了互斥锁后，若没有释放，其他 Goroutine 再想获取锁就会阻塞。

因此在加了互斥锁后，可以下意识加个 defer mutex.Unlock()，养成编码习惯

```go
func main() {
    total := 0
    defer func() {
        time.Sleep(time.Second)
        fmt.Println("total: ", total)
        fmt.Println("goroutines: ", runtime.NumGoroutine())
    }()

    var mutex sync.Mutex
    for i := 0; i < 10; i++ {
        go func() {
            mutex.Lock()  
            // 正常加锁后，可以下意识加个 defer mutex.Unlock()
            total += 1
        }()
    }
}
```

**同步锁使用不当**

如下例子中，wg.Add 的数量与 wg.Done 的数量不一致，就会导致 wg.Wait 阻塞。

```go
func handle(v int) {
    var wg sync.WaitGroup
    wg.Add(5)
    for i := 0; i < v; i++ {
        wg.Done()
    }
    wg.Wait()
}

func main() {
    defer func() {
        fmt.Println("goroutines: ", runtime.NumGoroutine())
    }()

    go handle(3)
    time.Sleep(time.Second)
}
```





## 23 Golang 里的逃逸分析是什么？怎么避免内存逃逸？



## 24 goroutine 和 kernel thread 之间什么关系



## 25 go 内存逃逸分析（分析了栈帧，讲五种例子，描述堆栈优缺点，点头）



## 26 defer recover 的问题



## 27 select可以用于什么？



## 28 优雅关闭 channel



## 29 Go 有异常类型吗？

**在 Go 没有异常类型，只有错误类型（Error）。**

Go 语言中虽然没有异常的概念，但是却有更为恐怖的 panic ，由于有了 recover，在一定程度上， panic 可以类比做异常。

Golang错误和异常（panic）是可以互相转换的：

1. **错误转异常**：比如程序逻辑上尝试请求某个URL，最多尝试三次，尝试三次的过程中请求失败是错误，尝试完第三次还不成功的话，失败就被提升为异常了。
2. **异常转错误**：比如panic触发的异常被recover恢复后，将返回值中error类型的变量进行赋值，以便上层函数继续走错误处理流程。



## 30 go 中为什么要用goroutine

线程其实分两种：

- 一种是传统意义的操作系统线程
- 一种是编程语言实现的用户态线程，也称为协程，在 Go 中就是 goroutine

因此，goroutine 的存在必然是为了换个方式解决操作系统线程的一些弊端 — **太重** 。

太重表现在如下几个方面：

**第一：创建和切换太重**

操作系统线程的创建和切换都需要进入内核，而进入内核所消耗的性能代价比较高，开销较大；

**第二：内存使用太重**

一方面，为**了尽量避免极端情况下操作系统线程栈的溢出，内核在创建操作系统线程时默认会为其分配一个较大的栈内存**（虚拟地址空间，内核并不会一开始就分配这么多的物理内存），然而在绝大多数情况下，系统线程远远用不了这么多内存，这导致了浪费；

另一方面，栈内存空间一旦创建和初始化完成之后其大小就不能再有变化，这决定了在某些特殊场景下系统线程栈还是有溢出的风险。

相对的，用户态的goroutine则轻量得多：

- **goroutine是用户态线程，其创建和切换都在用户代码中完成而无需进入操作系统内核，所以其开销要远远小于系统线程的创建和切换；**
- **goroutine启动时默认栈大小只有2k，这在多数情况下已经够用了，即使不够用，goroutine的栈也会自动扩大**，**同时，如果栈太大了过于浪费它还能自动收缩**，**这样既没有栈溢出的风险，也不会造成栈内存空间的大量浪费。**

##  

## 31 Go 的默认栈大小是多少？最大值多少？

 2KB

**为啥会有栈空间扩容**

由于当前的 **Go 的栈结构使用的是连续栈**，并且初始值才 2k 比较小，因此随着函数的调用层级加深，Go 的初始栈空间就可能不够用，不够用的话，就会触发栈空间的扩容。

**栈空间扩容啥时会触发**

编译器会为函数调用插入运行时检查`runtime.morestack`，它会在几乎所有的函数调用之前检查当前`goroutine` 的栈内存是否充足，如果当前栈需要扩容，会调用`runtime.newstack` 创建新的栈。

而新的栈空间，是旧栈空间大小（通过保存在`goroutine`中的`stack`信息里记录的栈区内存边界计算出来的）的两倍，但最大栈空间大小不能超过 `maxstacksize` ，也就是 **1G。**

**不管是扩容还是缩容，都是使用 `runtime.copystack` 函数来开辟新的栈空间，然后将旧栈的数据全部拷贝至新的栈空间，并调整原来指针的指向。**



## 32 GMP 为什么要有 P ？

### GM 模型是怎样的

在 Go v1.1 之前，实际上 GMP确实是没有 P 的，**所有的 M 线程都要从 全局队列中获取 G 来执行任务，为了避免冲突，从全局队列中获取 G 的时候，要先获取一把大锁。**

当一个程序的并发量比较小的时候，影响还不大，而当程序的并发量非常大的时候，这个全局队列会成为性能的瓶颈。

除此之外 ，**若直接把 G 从全局队列分配给 M，那么当 G 中当生系统调用或者其他阻塞性的操作时，M 会有一段时间处于挂起的状态**，此时又没有新创建线程的线程来代替该线程继续从队列中取出其他 G 来运行，从效率上其实会打折扣。

### P 带来的改变

加了 P 之后会带来什么改变呢？

- **每个 P 有自己的本地队列，大幅度的减轻了对全局队列的直接依赖，所带来的效果就是锁竞争的减少**。而 GM 模型的性能开销大头就是锁竞争。
- **当一个 M 中 运行的 G 发生阻塞性操作时，P 会重新选择一个 M，若没有 M 就新创建一个 M 来继续从 P 本地队列中取 G 来执行，提高运行效率。**
- 每个 P 相对的平衡上，在 GMP 模型中也实现了 **Work Stealing 算法**，如果 P 的本地队列为空，则会从全局队列或其他 P 的本地队列中窃取可运行的 G 来运行，减少空转，提高了资源利用率。



## 33 Go 中哪些动作会触发 runtime 调度？

**第一种：系统调用 SysCall**

**当你在 goroutine 进行一些 sleep 休眠、读取磁盘或者发送网络请求时，其实都会发生系统调用，进入操作系统内核。**

而一旦发生系统调用，就会直接触发 runtime 的调度，当前的 P 就会去找其他的 M 进行绑定，并取出 G 开始运行。

**第二种：等待锁、通道**

此外，在你的代码中，**若因为锁或者通道导致代码阻塞了，也会触发调度**。

**第三种：人工触发**

在代码中直接调用 runtime.Gosched 方法，也可以手动触发。



## 34 GMP 偷取 G 为什么不需要加锁？

P 从全局队列里取 G 的时候，由于可能有多个 P 同时取G 的情况，因此需要加锁，这很容易理解。然后 P 从本地队列里取 G 的时候，正常情况下，只有自己取 G，不用加锁也没关系。但问题就在于当有其他的 P 处于自旋状态的时候，就有可能来自己这边偷。

如此看来，P 的本地队列也有并发竞争的问题，可为什么网上的文章都说从 P 本地队列里的时候，也不用加锁呢？

难道是这些人，瞎说的？

其实啊，这些人说的并没有错，只是他们没把事情说清楚。

原来 P 从本地队列取 G 的这个操作，是一个 CAS 操作，它具有原子性，是由硬件直接支持的，不需要并发的竞争关系。

而我们常见的加锁操作来避免并发的竞争问题，是从操作系统层面来实现的。

因此 GMP 中偷取 G 的过程也是不需要加锁的噢。

CAS 的原子操作虽然可以让程序员变得简单，有时也要付出一定的代价，使用 CAS 有两个小问题：

1. 使用 CAS 为保证执行成功，程序需要用 for 循环不断尝试，直到成功才返回，因此如果 CAS 长时间不成功，就会阻塞其他硬件对CPU的访问，开销比较大。
2. 每次使用 CAS 会原子操作时，一般只能对一个共享变量做操作，若要对多个共享变量操作，循环 CAS 可能就不太做，不过应该可以通过把多个变量放在一个对象里来进行 CAS 操作。

为了让你对 CAS 有一个直观的理解，这里直接放上网上找的一段使用 atomic 包实现的 CAS 操作代码

```go
package main

import (
"fmt"
"sync"
"sync/atomic"
)

var (
    counter int32//计数器
    wg sync.WaitGroup //信号量
)

func main() {
    threadNum := 5 //1. 五个信号量
    wg.Add(threadNum) //2.开启5个线程
    for i := 0; i < threadNum; i++ {
        Go incCounter(i)
    }
    //3.等待子线程结束
    wg.Wait()
    fmt.Println(counter)
}

func incCounter(index int) {
    defer wg.Done()
    spinNum := 0
    for {
        //2.1原子操作
        old := counter
        ok := atomic.CompareAndSwapInt32(&counter, old, old+1)
        if ok {
            break
        } else {
            spinNum++
        }
    }
    fmt.Printf("thread,%d,spinnum,%d\n",index,spinNum)
}
```



## 35 说一下 GMP 模型的原理

###  什么是 GMP ？

- `G`：Goroutine，也就是 go 里的协程，**是用户态的轻量级线程**，具体可以创建多个 goroutine ，取决你的内存有多大，一个 goroutine 大概需要 4k 内存，只要你不是在 32 位的机器上，那么创建个几百万个的 goroutine 应该没有问题。
- `M`：Thread，也就是操作**系统线程**，go runtime 最多允许创建 10000 个操作系统线程，超过了就会抛出异常
- `P`：**Processor，处理器，数量默认等于开机器的CPU核心数**，若想调小，可以通过 GOMAXPROCS 这个环境变量设置。

### GMP 核心

#### 两个队列

在整个 Go 调度器的生命周期中，存在着两个非常重要的队列：

- 全局队列（Global Queue）：全局只有一个
- 本地队列（Local Queue）：每个 P 都会维护一个本地队列

当你执行 `go func()` 创建一个 goroutine 时，会优选将该协程放入到当前 P 的本地队列中等待被 P 选中执行。

但若当前 P 的本地队列任务太多了，已经存放不下了，那么这个 goroutine 就只能放入到全局队列中。

#### 两种调度

一个协程得以运行，需要同时满足以下两个条件：

1. P 已经和某个线程进行绑定，这样才能参考操作系统的调度获得 CPU 时间
2. P 已经从队列中（可以是本地队列，也可以是全局队列，甚至是从其他 P 的队列）取到该协程

第一个条件就是 **操作系统调度**，而第二个其实就是 **Go 里的调度器**。

#### 操作系统调度

假设一台机器上有两个 CPU 核心，意味着，同时在同一时间里，只能有两个线程运行着。

可如果该机器上实际开启了 4 个线程，要是先执行一个线程完再执行另一个线程，那么当某一个线程因为一些阻塞性的系统调用而阻塞时，CPU 的时间就会因此而白白浪费掉了。

更合适的做法是，使用 **操作系统调度策略**，设定一个调度周期，假设是 10ms （毫秒），那在一个周期里，每个线程都平均分，都只能得到 2.5ms 的CPU 运行时间。

可如果机器上有 1000 个线程呢？难道每个线程都分个 0.01 ms （也就是 10 微秒）吗？

要知道，CPU 从 A 线程切换到 B 线程，是有巨大的时间浪费在线程上下文的切换，如果切换得太频繁，就会有大量的 CPU 时间白白浪费。

![Go 语言面试题 100 讲之 017篇：说一下 GMP 模型的原理插图](http://image.iswbm.com/20210904140447.png)

因此，通常会限制最小的时间片的长度，假设为 2ms，受此调整，现在调度周期就会变成 2*1000 = 2s 。

#### Go调度器

在 Go 中需要用到调度的，无非是如下几种：

**将 P 绑定到一个合适的 M **

P 本身不能直接运行 G，只将 P 跟 M 绑定后，才能执行 G。

假设 P1 当前正绑定在 M1 上运行 G1，此时 G1 内部发生了一次系统调度后，P1 就会与 M1 进行解绑，然后再从空闲的线程队列中再寻找一个来绑定，假设绑定的是 M2，可如果没有空闲的线程呢？那没办法，只能创建一个新的线程再进行绑定。

绑定后，就会再从本地的队列中寻找 G 来执行（如果没找到，就会去其他队列找，上面已经讲过，不再赘述）。

过了一段时间后，之前 M1 上 G1 发生的系统调用结束后，M1 会去找原先自己的搭档 P1（它自己会记录），如果自己的老搭档也刚好空闲着，就可以再次合作进行绑定，接着运行 G1 未完成的工作。

可不幸的是，P1 已经找到了新的合作伙伴 M2，暂时没空搭理 M1 。

M1 联系不上 P1，只能去寻找有没有其他空闲的 P ，如果所有的 P 都被绑定着，说明现在任务非常繁重，想完成任务只能排队慢慢等。

于是，M1 上的 G1 就会被标记为 Runable ，放到全局队列中，而 M1 自身也会因为没有 P 可以绑定而进入休眠状态，如果长时间休眠等待 则会 GC 回收销毁

**为 P 选中一个 G 来执行**

P 就像是一个流水线工人，而 P 的本地队列就是流水线，G 是流水线上的零件。而 Go 调度器就是流水线组长，负责监督工人的是不是有在努力的工作。

完成一个 G 后，P 就得立马接着从队列中拿到下一个 G，继续干活。

遇到手脚麻利的 P ，干完了自己的活，本想着可以偷懒一会，没想到却被组长发现了，立马就从全局队列中拿了些新的 G 交到 P 的手里。

天真的 P 以为只要把 全局队列中的 G 的也干完了，就肯定 能休息了吧？

当 P 又快手快脚的把全局队列中的 G 也都干完的时候，P 非常得意，心想：终于可以休息会了。

没想到又被眼尖的组长察觉到了：不错啊，小 P，手脚挺麻利的。看看其他人，还那么多活没干完。真是拖后腿。可谁让咱是一个团队的呢，要有集体荣誉感，你能者多劳。

说完，就把其他人的 G 放到了我的工作上。。。

### 调度器的设计策略

#### 复用线程

避免频繁的创建、销毁线程，而是对线程的复用。

**1）work stealing 机制**

**当本线程无可运行的 G 时，尝试从其他线程绑定的 P 偷取 G，而不是销毁线程。**

**2）hand off 机制**

**当本线程因为 G 进行系统调用阻塞时，线程释放绑定的 P，把 P 转移给其他空闲的线程执行。**

#### 利用并行

GOMAXPROCS 设置 P 的数量，最多有 GOMAXPROCS 个线程分布在多个 CPU 上同时运行。GOMAXPROCS 也限制了并发的程度，比如 GOMAXPROCS = 核数/2，则最多利用了一半的 CPU 核进行并行。

#### 抢占调度

在 Go 中，一个 goroutine 最多占用 CPU 10ms，防止其他 goroutine 被饿死，这是协作式抢占调度。

而在 go 1.14+ ，Go 开始支持基于信号的真抢占调度了。

#### 全局 G 队列

在新的调度器中依然有全局 G 队列，但功能已经被弱化了，当 M 执行 work stealing 从其他 P 偷不到 G 时，它可以从全局 G 队列获取 G。





# gin



### 聊聊你对gin的理解

gin是一个go的微框架，封装优雅，API友好。快速灵活。容错方便等特点。

其实对于go而言，对web框架的依赖远比Python、Java之类的小。本身的`net/http`足够简单，而且性能也非常不错，大部分的框架都是对`net/http`的高阶封装。所以gin框架更像是一些常用函数或者工具的集合。使用gin框架发开发，可以提升效率，并同意团队的编码风格。

### gin的路由组件为什么高性能

#### 路由树

gin使用高性能路由库`httprouter`[5](#fn-5)

在Gin框架中，路由规则被分成了9课前缀树，每一个HTTP Method对应一颗前缀树，树的节点按照URL中的 / 符号进行层级划分

#### gin.RouterGroup

RouterGroup是对路由树的包装，所有的路由规则最终都是由它来进行管理。Engine结构体继承了RouterGroup，所以Engine直接具备了RouterGroup所有的路由管理功能。

### gin数据绑定

gin提供了很方便的数据绑定功能，**可以将用户传过来的参数自动跟我们定义的结构体绑定在一起。**

这是也我选择用gin的重要原因。

### gin数据验证

在上面数据绑定的基础上，gin还提供了数据校验的方法。gin的数据验证是和数据绑定结合在一起的。只需要在数据绑定的结构体成员变量的标签添加`binding`规则即可。这又省了大量的验证工作，对用惯AspCoreMVC、Spring MVC的程序员来说是完美的替代框架。

### gin的中间件

gin中间件利用函数调用栈`后进先出`的特点，完成中间件在自定义处理函数完成后的处理操作。



# 数据库



## 事务

## 索引

## 事务是怎么实现的?



# istio

