Kubernetes一直是当今业界的流行语，也是最好的编排工具。它吸引了许多想要提升自己职业生涯的经验丰富的专业人士。Huwaei，Pokemon，Box，eBay，Ing，Yahoo Japan，SAP，纽约时报，Open AI，Sound Cloud等跨国公司也使用Kubernetes。我相信你已经知道这些事实，这也是促使你打开这个Kubernetes面试问题文章原因。

在这篇关于Kubernetes面试问题的文章中，我将讨论在面试中提出的与Kubernetes相关的最重要问题。因此，为了您的理解，我将此博客分为以下4个部分：

- Kubernetes基本面试问题
- 基于架构的面试问题
- 基于场景的面试问题
- 多项选择题

所以让我们开始吧!!

## 基本的Kubernetes面试问题

这部分问题将包含您需要了解的与[Kubernetes](https://www.kubernetes.org.cn/)工作相关的所有基本问题。

## Q1。Kubernetes与Docker Swarm的区别如何？

![img](https://pic4.zhimg.com/80/v2-4cb53bef7469844ce25da36b5676f14f_hd.jpg)

## Q2。什么是Kubernetes？

![img](https://pic4.zhimg.com/80/v2-7da8172e18d439e6187d43df03137cc3_hd.jpg)

[Kubernetes](https://link.zhihu.com/?target=https%3A//www.edureka.co/blog/what-is-kubernetes-container-orchestration%3Futm_source%3Dmedium%26utm_medium%3Dcontent-link%26utm_campaign%3Dkubernetes-interview-questions%26source%3Dpost_page---------------------------)是一个开源容器管理工具，负责容器部署，容器扩缩容以及负载平衡。作为Google的创意之作，它提供了出色的社区，并与所有云提供商合作。因此，我们可以说Kubernetes不是*一个容器化平台，而是一个多容器管理解决方案。*

## Q3。Kubernetes与Docker有什么关系？

众所周知，Docker提供容器的生命周期管理，Docker镜像构建运行时容器。但是，由于这些单独的容器必须通信，因此使用Kubernetes。因此，我们说Docker构建容器，这些容器通过Kubernetes相互通信。因此，可以使用Kubernetes手动关联和编排在多个主机上运行的容器。

## Q4。在主机和容器上部署应用程序有什么区别？

![img](https://pic2.zhimg.com/80/v2-5f94533f6a374a833f4b1f40ea1ef905_hd.jpg)

请参考上图。左侧架构表示在主机上部署应用程序。因此，这种架构将具有操作系统，然后操作系统将具有内核，该内核将在应用程序所需的操作系统上安装各种库。因此，在这种框架中，您可以拥有n个应用程序，**并且所有应用程序将共享该操作系统中存在的库**，而在容器中部署应用程序时，体系结构则略有不同。

这种架构将有一个内核，这是唯一一个在所有应用程序之间唯一共同的东西。因此，如果有一个需要Java的特定应用程序，那么我们将获得访问Java的特定应用程序，如果有另一个需要Python的应用程序，则只有该特定应用程序才能访问Python。

您可以在图表右侧看到的各个块基本上是容器化的，并且这些块与其他应用程序隔离。因此，应用程序具有与系统其余部分隔离的必要库和二进制文件，并且不能被任何其他应用程序侵占。

**虚拟机和容器最大的区别是容器更快并且更轻量级——与虚拟机运行在完整的操作系统之上相比，容器会共享其所在主机的操作系统/内核。**

## Q5。什么是Container Orchestration？

考虑一个应用程序有5-6个微服务的场景。现在，这些微服务被放在单独的容器中，但如果没有容器编排就无法进行通信。因此，由于编排意味着所有乐器在音乐中和谐共处，所以类似的容器编排意味着各个容器中的所有服务协同工作以满足单个服务器的需求。

## Q6。Container Orchestration需要什么？

考虑到你有5-6个微服务用于执行各种任务的单个应用程序，所有这些微服务都放在容器中。现在，为了确保这些容器彼此通信，我们需要容器编排。

![img](https://pic4.zhimg.com/80/v2-eaaa8de78b63464ffdea52418eddbae7_hd.jpg)

正如您在上图中所看到的，在没有使用容器编排的情况下，还存在许多挑战。因此，为了克服这些挑战，容器编排就到位了。

## Q7。Kubernetes有什么特点？

Kubernetes的功能如下：

![img](https://pic3.zhimg.com/80/v2-0ab7e8883908e70ee463cec54744424e_hd.jpg)

## Q8 Kubernetes如何简化容器化部署？

由于典型应用程序将具有跨多个主机运行的容器集群，因此所有这些容器都需要相互通信。因此，要做到这一点，你需要一些能够负载平衡，扩展和监控容器的东西。由于Kubernetes与云无关并且可以在任何公共/私有提供商上运行，因此必须是您简化容器化部署的选择。

## Q9 您对Kubernetes的集群了解多少？

Kubernetes背后的基础是我们可以实施所需的状态管理，我的意思是我们可以提供特定配置的集群服务，并且集群服务将在基础架构中运行并运行该配置。

![img](https://pic4.zhimg.com/80/v2-34f3959acd587968133f4ddf6f90b2b7_hd.jpg)

因此，正如您在上图中所看到的，部署文件将具有提供给集群服务所需的所有配置。现在，部署文件将被提供给API，然后由集群服务决定如何在环境中安排这些pod，并确保正确运行的pod数量。

因此，位于服务前面的API，工作节点和节点运行的Kubelet进程，共同构成了Kubernetes集群。

## Q10 什么是Google容器引擎？

**Google Container Engine（GKE）**是Docker容器和集群的开源管理平台。这个基于Kubernetes的引擎仅支持在Google的公共云服务中运行的群集。

## Q11 什么是Heapster？

Heapster是由每个节点上运行的Kubelet提供的集群范围的数据聚合器。此容器管理工具在Kubernetes集群上本机支持，并作为pod运行，就像集群中的任何其他pod一样。因此，它基本上发现集群中的所有节点，并通过机上Kubernetes代理查询集群中Kubernetes节点的使用信息。

## Q12 什么是Minikube？

Minikube是一种工具，可以在本地轻松运行Kubernetes。这将在虚拟机中运行单节点Kubernetes群集。

## Q13 什么是Kubectl？

Kubectl是一个平台，您可以使用该平台将命令传递给集群。因此，它基本上为CLI提供了针对Kubernetes集群运行命令的方法，以及创建和管理Kubernetes组件的各种方法。

## Q14 什么是Kubelet？

这是一个代理服务，它在每个节点上运行，并使从服务器与主服务器通信。因此，Kubelet处理PodSpec中提供给它的容器的描述，并确保PodSpec中描述的容器运行正常。

## Q15 你对Kubernetes的一个节点有什么了解？

![img](https://pic2.zhimg.com/80/v2-ed5149de3921a6b4d208c6cf3f310f2d_hd.jpg)

## 基于架构的Kubernetes访谈问题

这部分问题将涉及与Kubernetes架构相关的问题。

## Q1。Kubernetes Architecture的不同组件有哪些？

Kubernetes Architecture主要有两个组件 – 主节点和工作节点。如下图所示，master和worker节点中包含许多内置组件。主节点具有kube-controller-manager，kube-apiserver，kube-scheduler等。而工作节点具有在每个节点上运行的kubelet和kube-proxy。

![img](https://pic3.zhimg.com/80/v2-fd9957557b9db49c5c2696a673b32e46_hd.jpg)

## Q2 你对Kube-proxy有什么了解？

Kube-proxy可以在每个节点上运行，并且可以跨后端网络服务进行简单的TCP / UDP数据包转发。基本上，它是一个网络代理，它反映了每个节点上Kubernetes API中配置的服务。因此，Docker可链接的兼容环境变量提供由代理打开的群集IP和端口。

## Q3 您能否介绍一下Kubernetes中主节点的工作情况？

Kubernetes master控制容器存在的节点和节点内部。现在，这些单独的容器包含在容器内部和每个容器内部，您可以根据配置和要求拥有不同数量的容器。因此，如果必须部署pod，则可以使用用户界面或命令行界面部署它们。然后，在节点上调度这些pod，并根据资源需求，将pod分配给这些节点。kube-apiserver确保在Kubernetes节点和主组件之间建立通信。

![img](https://pic2.zhimg.com/80/v2-cadad4807f932459e16ec22c2f3a4239_hd.jpg)

## Q4 kube-apiserver和kube-scheduler的作用是什么？

kube -apiserver遵循横向扩展架构，是主节点控制面板的前端。这将公开Kubernetes主节点组件的所有API，并负责在Kubernetes节点和Kubernetes主组件之间建立通信。

kube-scheduler负责工作节点上工作负载的分配和管理。因此，它根据资源需求选择最合适的节点来运行未调度的pod，并跟踪资源利用率。它确保不在已满的节点上调度工作负载。

## Q5 你能简要介绍一下Kubernetes控制管理器吗？

多个控制器进程在主节点上运行，但是一起编译为单个进程运行，即Kubernetes控制器管理器。因此，Controller Manager是一个嵌入控制器并执行命名空间创建和垃圾收集的守护程序。它拥有责任并与API服务器通信以管理端点。

因此，主节点上运行的不同类型的控制器管理器是：

![img](https://pic3.zhimg.com/80/v2-7c4a4bd031e0d7627411a4bd9506c5a2_hd.jpg)

## Q6 什么是ETCD？

Etcd是用Go编程语言编写的，是一个分布式键值存储，用于协调分布式工作。因此，Etcd存储Kubernetes集群的配置数据，表示在任何给定时间点的集群状态。

## Q7 Kubernetes有哪些不同类型的服务？

以下是使用的不同类型的服务：

![img](https://pic3.zhimg.com/80/v2-f0964d362e748d9cda3e4ed629125fd6_hd.jpg)

## Q8 你对Kubernetes的负载均衡器有什么了解？

负载均衡器是暴露服务的最常见和标准方式之一。根据工作环境使用两种类型的负载均衡器，即*内部负载均衡器*或*外部负载均衡器*。内部负载均衡器自动平衡负载并使用所需配置分配容器，而外部负载均衡器将流量从外部负载引导至后端容器。

## Q9 什么是Ingress网络，它是如何工作的？

[Ingress网络](https://link.zhihu.com/?target=https%3A//www.edureka.co/blog/kubernetes-networking%3Futm_source%3Dmedium%26utm_medium%3Dcontent-link%26utm_campaign%3Dkubernetes-interview-questions%26source%3Dpost_page---------------------------)是一组规则，充当Kubernetes集群的入口点。这允许入站连接，可以将其配置为通过可访问的URL，负载平衡流量或通过提供基于名称的虚拟主机从外部提供服务。因此，Ingress是一个API对象，通常通过HTTP管理集群中服务的外部访问，是暴露服务的最有效方式。

现在，让我以一个例子向您解释Ingress网络的工作。

有2个节点具有带有Linux桥接器的pod和根网络命名空间。除此之外，还有一个名为flannel0（网络插件）的新虚拟以太网设备被添加到根网络中。

现在，假设我们希望数据包从pod1流向pod 4.请参阅下图。

![img](https://pic4.zhimg.com/80/v2-bd475e9c34e32e21e1db2f7c6307484f_hd.jpg)

- 因此，数据包将pod1的网络保留在eth0，并进入veth0的根网络。
- 然后它被传递给cbr0，这使得ARP请求找到目的地，并且发现该节点上没有人具有目的地IP地址。
- 因此，桥接器将数据包发送到flannel0，因为节点的路由表配置了flannel0。
- 现在，flannel守护程序与Kubernetes的API服务器通信，以了解所有pod IP及其各自的节点，以创建pods IP到节点IP的映射。
- 网络插件将此数据包封装在UDP数据包中，其中额外的标头将源和目标IP更改为各自的节点，并通过eth0发送此数据包。
- 现在，由于路由表已经知道如何在节点之间路由流量，因此它将数据包发送到目标节点2。
- 数据包到达node2的eth0并返回到flannel0以解封装并在根网络命名空间中将其发回。
- 同样，数据包被转发到Linux网桥以发出ARP请求以找出属于veth1的IP。
- 数据包最终穿过根网络并到达目标Pod4。

## Q10 您对云控制器管理器有何了解？

Cloud Controller Manager负责持久存储，网络路由，从核心Kubernetes特定代码中抽象出特定于云的代码，以及管理与底层云服务的通信。它可能会分成几个不同的容器，具体取决于您运行的是哪个云平台，然后它可以使云供应商和Kubernetes代码在没有任何相互依赖的情况下开发。因此，云供应商开发他们的代码并在运行Kubernetes时与Kubernetes云控制器管理器连接。

各种类型的云控制器管理器如下：

![img](https://pic2.zhimg.com/80/v2-cfdbb58b5fa3ab0752c5c4281a890ba9_hd.jpg)

## Q11 什么是Container资源监控？

对于用户而言，了解应用程序的性能和所有不同抽象层的资源利用率非常重要，Kubernetes通过在容器，pod，服务和整个集群等不同级别创建抽象来考虑集群的管理。现在，可以监视每个级别，这只是容器资源监视。

各种容器资源监控工具如下：

![img](https://pic1.zhimg.com/80/v2-1c964cbc76ef3c262fef7de8887967b4_hd.jpg)

## Q12 Replica Set 和 Replication Controller之间有什么区别？

Replica Set 和 Replication Controller几乎完全相同。它们都确保在任何给定时间运行指定数量的pod副本。不同之处在于复制pod使用的选择器。Replica Set使用基于集合的选择器，而Replication Controller使用基于权限的选择器。

- **Equity-Based选择器：**这种类型的选择器允许按标签键和值进行过滤。因此，在外行术语中，基于Equity的选择器将仅查找与标签具有完全相同短语的pod。 **示例**：假设您的标签键表示app = nginx，那么，使用此选择器，您只能查找标签应用程序等于nginx的那些pod。
- **Selector-Based选择器：**此类型的选择器允许根据一组值过滤键。因此，换句话说，基于Selector的选择器将查找已在集合中提及其标签的pod。 **示例：**假设您的标签键在（nginx，NPS，Apache）中显示应用程序。然后，使用此选择器，如果您的应用程序等于任何nginx，NPS或Apache，则选择器将其视为真实结果。

## Q13 什么是Headless Service？

Headless Service类似于“普通”服务，但没有群集IP。此服务使您可以直接访问pod，而无需通过代理访问它。

## Q14 使用Kubernetes时可以采取哪些最佳安全措施？

以下是使用Kubernetes时可以遵循的最佳安全措施：

![img](https://pic3.zhimg.com/80/v2-aed250fc6eaf6c752267982fff782706_hd.jpg)

## Q15 什么是集群联邦？

在联邦集群的帮助下，可以将多个Kubernetes集群作为单个集群进行管理。因此，您可以在数据中心/云中创建多个Kubernetes集群，并使用联邦来在一个位置控制/管理它们。

联合集群可以通过执行以下两项操作来实现此目的。请参考下图。

![img](https://pic3.zhimg.com/80/v2-657422f419a40db3668545dc1fc974d2_hd.jpg)

## 基于场景的面试问题

这部分问题将包含您在面试中可能遇到的各种基于场景的问题。

## **场景1：**

假设一家基于单一架构的公司处理众多产品。现在，随着公司在当今的扩展行业的扩展，他们的单一架构开始引发问题。

*您如何看待公司从单一服务转向微服务并部署其服务容器？*

**解：**

由于公司的目标是从单一应用程序转向微服务，它们最终可以逐个构建，并行构建，只需在后台切换配置。然后他们可以将这些内置微服务放在Kubernetes平台上。因此，他们可以从一次或两次迁移服务开始，并监控它们以确保一切运行稳定。一旦他们觉得一切顺利，他们就可以将其余的应用程序迁移到他们的Kubernetes集群中。

## **场景2：**

考虑一家拥有分布式系统的跨国公司，拥有大量数据中心，虚拟机和许多从事各种任务的员工。

*您认为这样* *的公司如何以与Kubernetes一致的方式管理所有任务？*

**解：**

正如我们所有人都知道IT部门推出了数千个容器，其任务在分布式系统中遍布全球众多节点。

在这种情况下，公司可以使用能够为基于云的应用程序提供敏捷性，横向扩展功能和DevOps实践的东西。

因此，该公司可以使用Kubernetes来定制他们的调度架构并支持多种容器格式。这使得容器任务之间的亲和性成为可能，从而提供更高的效率，并为各种容器网络解决方案和容器存储提供广泛支持。

## **场景3：**

考虑一种情况，即公司希望通过维持最低成本来提高其效率和技术运营速度。

*您认为公司将如何实现这一目标？*

**解：**

公司可以通过构建CI/CD管道来实现DevOps方法，但是这里可能出现的一个问题是配置可能需要一段时间才能启动并运行。因此，在实施CI/CD管道之后，公司的下一步应该是在云环境中工作。一旦他们开始处理云环境，他们就可以在集群上安排容器，并可以在Kubernetes的帮助下进行协调。这种方法将有助于公司缩短部署时间，并在各种环境中加快速度。

## **场景4：**

假设一家公司想要修改它的部署方法，并希望建立一个更具可扩展性和响应性的平台。

*您如何看待这家公司能够实现这一目标以满足客户需求？*

**解：**

为了给数百万客户提供他们期望的数字体验，公司需要一个可扩展且响应迅速的平台，以便他们能够快速地将数据发送到客户网站。现在，要做到这一点，公司应该从他们的私有数据中心（如果他们使用任何）转移到任何云环境，如[AWS](https://link.zhihu.com/?target=https%3A//www.edureka.co/blog/amazon-aws-tutorial/%3Futm_source%3Dmedium%26utm_medium%3Dcontent-link%26utm_campaign%3Dkubernetes-interview-questions%26source%3Dpost_page---------------------------)。不仅如此，他们还应该实现微服务架构，以便他们可以开始使用Docker容器。一旦他们准备好基础框架，他们就可以开始使用最好的编排平台，即Kubernetes。这将使团队能够自主地构建应用程序并快速交付它们。

## **场景5：**

考虑一家拥有非常分散的系统的跨国公司，期待解决整体代码库问题。

*您认为公司如何解决他们的问题？*

**解**

那么，为了解决这个问题，我们可以将他们的单片代码库转移到微服务设计，然后每个微服务都可以被视为一个容器。因此，所有这些容器都可以在Kubernetes的帮助下进行部署和协调。

## **场景6：**

我们所有人都知道，从单片到微服务的转变解决了开发方面的问题，但却增加了部署方面的问题。

*公司如何解决部署方面的问题？*

**解**

团队可以试验容器编排平台，例如Kubernetes，并在数据中心运行。因此，通过这种方式，公司可以生成模板化应用程序，在五分钟内部署它，并在此时将实际实例集中在暂存环境中。这种Kubernetes项目将有数十个并行运行的微服务，以提高生产率，即使节点出现故障，也可以立即重新安排，而不会影响性能。

## **场景7：**

假设一家公司希望通过采用新技术来优化其工作负载的分配。

*公司如何有效地实现这种资源分配？*

**解**

这个问题的解决方案就是Kubernetes。Kubernetes确保资源得到有效优化，并且只使用特定应用程序所需的那些资源。因此，通过使用最佳容器编排工具，公司可以有效地实现资源分配。

## **场景8：**

考虑一家拼车公司希望通过同时扩展其平台来增加服务器数量。

*您认为公司如何处理服务器及其安装？*

**解**

公司可以采用集装箱化的概念。一旦他们将所有应用程序部署到容器中，他们就可以使用Kubernetes进行编排，并使用像Prometheus这样的容器监视工具来监视容器中的操作。因此，利用容器的这种使用，在数据中心中为它们提供更好的容量规划，因为它们现在将受到更少的限制，因为服务和它们运行的硬件之间存在抽象。

## **场景9：**

考虑一种情况，公司希望向具有各种环境的客户提供所有必需的分发。

*您认为他们如何以动态的方式实现这一关键目标？*

**解**

该公司可以使用Docker环境，组建一个横截面团队，使用Kubernetes构建Web应用程序。这种框架将帮助公司实现在最短的时间内将所需产品投入生产的目标。因此，在这样的机器运行的情况下，公司可以向所有具有各种环境的客户发放电子邮件。

## **场景10**：

假设公司希望在不同的云基础架构上运行各种工作负载，从裸机到公共云。

*公司将如何在不同界面的存在下实现这一目标？*

**解**

该公司可以将其基础设施分解为微服务，然后采用Kubernetes。这将使公司在不同的云基础架构上运行各种工作负载。

## 多项选择面试问题

这部分问题将包括多项面试问题，这些问题在面试中经常被问到。

## **Q1 什么是Kubernetes集群中的minions？**

1. 它们是主节点的组件。
2. 它们是集群的工作节点。[答案]
3. 他们正在监控kubernetes中广泛使用的引擎。
4. 他们是docker容器服务。

## **Q2 Kubernetes集群数据存储在以下哪个位置？**

1. KUBE-API服务器
2. Kubelet
3. ETCD [答案]
4. 以上都不是

## **Q3 哪个是Kubernetes控制器？**

1. ReplicaSet
2. Deployment
3. Rolling Updates
4. ReplicaSet和Deployment [答案]

## **Q4 以下哪个是核心Kubernetes对象？**

1. Pods
2. Services
3. Volumes
4. 以上所有[答案]

## **Q5。Kubernetes Network代理在哪个节点上运行？**

1. Master Node
2. Worker Node
3. 所有节点[答案]
4. 以上都不是

## **Q6。** **节点控制器的职责是什么？**

1. 将CIDR块分配给节点
2. 维护节点列表
3. 监视节点的运行状况
4. 以上所有[答案]

## **Q7。Replication Controller的职责是什么？**

1. 使用单个命令更新或删除多个pod
2. 有助于达到理想状态
3. 如果现有Pod崩溃，则创建新Pod
4. 以上所有[答案]

## **Q8。如何在没有选择器的情况下定义服务？**

1. 指定外部名称[答案]
2. 指定具有IP地址和端口的端点
3. 只需指定IP地址即可
4. 指定标签和api版本

## **Q9。1.8版本的Kubernetes引入了什么？**

1. Taints and Tolerations [答案]
2. Cluster level Logging
3. Secrets
4. Federated Clusters

## **Q10。** **Kubelet** **调用的处理检查容器的IP地址是否打开的**程序是**？**

1. HTTPGetAction
2. ExecAction
3. TCPSocketAction [答案]
4. 以上都不是







# Kubernetes 的组成？

kubectl：客户端命令行工具，作为整个系统的操作入口。

kube-apiserver: 以REST API服务形式提供接口，作为整个系统的控制入口。

kube-conroller-manager:执行整个系统的后台任务，包括节点的状态状况，pod个数，pods和service的关联等。

kube-scheduler：负责节点的资源管理，接收来自kube-apiserver创建pods任务，并分配到某个节点。

etcd:复制节点间的服务发现和配置共享。

kube-proxy:运行在每个计算节点上，负责pod网络代理，定时从etcd获取service信息来做相应的策略。

kubelet：运行在每个计算节点上，作为agent，接收分配该节点的pods任务及管理容器，周期性获取容器状态，反馈给kube-apiserver

DNS：一个可选的DNS服务，用于为每个serivce对象创建DNS记录，这样所有的pod就可以通过DNS访问服务。





# k8s 创建一个pod的详细流程，涉及的组件怎么通信的？(非常重要)

这道题考察的是 k8s 内部组件通信。

k8s 创建一个 Pod 的详细流程如下： 

------

(1) 客户端提交创建请求，可以通过 `api-server` 提供的 `restful` 接口，或者是通过 `kubectl` 命令行工具，支持的数据类型包括 JSON 和 YAML。

(2) `api-server` 处理用户请求，将 pod 信息存储至 `etcd` 中。

(3) `kube-scheduler` 通过 `api-server` 提供的接口监控到未绑定的 pod，尝试为 pod 分配 node 节点，主要分为两个阶段，预选阶段和优选阶段，其中预选阶段是遍历所有的 node 节点，根据策略筛选出候选节点，而优选阶段是在第一步的基础上，为每一个候选节点进行打分，分数最高者胜出。

(4) 选择分数最高的节点，进行 `pod binding` 操作，并将结果存储至 `etcd` 中。

(5) 随后目标节点的 `kubelet` 进程通过 `api-server` 提供的接口监测到 `kube-scheduler` 产生的 pod 绑定事件，然后从 etcd 获取 pod 清单，下载镜像并启动容器。

------

整个事件流可以参考下图：



![img](https:////upload-images.jianshu.io/upload_images/16605471-363ddd93976fbd60.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/759/format/webp)

pod 创建事件流





# k8s 架构体系了解吗？简单描述下

这道题主要考察 k8s 体系，涉及的范围其实太广泛，可以从本身 k8s 组件、存储、网络、监控等方面阐述，当时我主要将 k8s 的每个组件功能都大概说了一下。

**Master节点**

Master节点主要有四个组件，分别是：api-server、controller-manager、kube-scheduler 和 etcd。

***api-server***

------

kube-apiserver 作为 k8s 集群的核心，负责整个集群功能模块的交互和通信，集群内的各个功能模块如 kubelet、controller、scheduler 等都通过 api-server 提供的接口将信息存入到 `etcd` 中，当需要这些信息时，又通过 api-server 提供的 restful 接口，如get、watch 接口来获取，从而实现整个 k8s 集群功能模块的数据交互。

------

***controller-manager***

------

controller-manager 作为 k8s 集群的管理控制中心，负责集群内 Node、Namespace、Service、Token、Replication 等资源对象的管理，**使集群内的资源对象维持在预期的工作状态。**

**每一个 controller 通过 api-server 提供的 restful 接口实时监控集群内每个资源对象的状态**，当发生故障，导致资源对象的工作状态发生变化，就进行干预，尝试将资源对象从当前状态恢复为预期的工作状态，常见的 controller 有 Namespace Controller、Node Controller、Service Controller、ServiceAccount Controller、Token Controller、ResourceQuote Controller、Replication Controller等。

------

***kube-scheduler***

------

kube-scheduler 简单理解为**通过特定的调度算法和策略为待调度的 Pod 列表中的每个 Pod 选择一个最合适的节点进行调度**，调度主要分为两个阶段，预选阶段和优选阶段，其中预选阶段是遍历所有的 node 节点，根据策略和限制筛选出候选节点，优选阶段是在第一步的基础上，通过相应的策略为每一个候选节点进行打分，分数最高者胜出，随后目标节点的 kubelet 进程通过 api-server 提供的接口监控到 kube-scheduler 产生的 pod 绑定事件，从 etcd 中获取 Pod 的清单，然后下载镜像，启动容器。

**预选阶段的策略有：**

(1) MatchNodeSelector：判断节点的 label 是否满足 Pod 的 nodeSelector 属性值。

(2) PodFitResource：判断节点的资源是否满足 Pod 的需求，批判的标准是：当前节点已运行的所有 Pod 的 request值 + 待调度的 Pod 的 request 值是否超过节点的资源容量。

(3) PodFitHostName：判断节点的主机名称是否满足 Pod 的 nodeName 属性值。

(4) PodFitHostPort：判断 Pod 的端口所映射的节点端口是否被节点其他 Pod 所占用。

(5) CheckNodeMemoryPressure：判断 Pod 是否可以调度到内存有压力的节点，这取决于 Pod 的 Qos 配置，如果是 BestEffort（尽量满足，优先级最低），则不允许调度。

(6) CheckNodeDiskPressure：如果当前节点磁盘有压力，则不允许调度。

**优选阶段的策略有：**

(1) SelectorSpreadPriority：尽量减少节点上同属一个 SVC/RC/RS 的 Pod 副本数，为了更好的实现容灾，对于同属一个 SVC/RC/RS 的 Pod 实例，应尽量调度到不同的 node 节点。

(2) LeastRequestPriority：优先调度到请求资源较少的节点，节点的优先级由节点的空闲资源与节点总容量的比值决定的，即（节点总容量 - 已经运行的 Pod 所需资源）/ 节点总容量，CPU 和 Memory 具有相同的权重，最终的值由这两部分组成。

(3) BalancedResourceAllocation：该策略不能单独使用，必须和 LeaseRequestPriority 策略一起结合使用，尽量调度到 CPU 和 Memory 使用均衡的节点上。

------

***ETCD\***

------

强一致性的键值对存储，k8s 集群中的所有资源对象都存储在 etcd 中。

------

**Node节点**

node节点主要有三个组件：分别是 kubelet、kube-proxy 和 容器运行时 docker 或者 rkt。

***kubelet\***

------

在 k8s 集群中，**每个 node 节点都会运行一个 kubelet 进程**，该进程用来处理 Master 节点下达到该节点的任务，同时，**通过 api-server 提供的接口定期向 Master 节点报告自身的资源使用情况**，并通过 cadvisor 组件监控节点和容器的使用情况。

------

***kube-proxy***

------

kube-proxy 就是一个智能的软件负载均衡器，**将 service 的请求转发到后端具体的 Pod 实例上**，并提供负载均衡和会话保持机制，目前有三种工作模式，分别是：用户模式（userspace）、iptables 模式和 IPVS 模式。

------

***容器运行时——docker\***

------

负责管理 node 节点上的所有容器和容器 IP 的分配。









## 1、 k8s是什么？请说出你的了解？ 

 答：Kubenetes是一个针对容器应用，进行自动部署，弹性伸缩和管理的开源系统。主要功能是生产环境中的容器编排。 K8S是Google公司推出的，它来源于由Google公司内部使用了15年的Borg系统，集结了Borg的精华。 

## 2、 K8s架构的组成是什么？

答：和大多数分布式系统一样，K8S集群至少需要一个主节点（Master）和多个计算节点（Node）。

主节点主要用于暴露API，调度部署和节点的管理； 计算节点运行一个容器运行环境，一般是docker环境（类似docker环境的还有rkt），同时运行一个K8s的代理（kubelet）用于和master通信。计算节点也会运行一些额外的组件，像记录日志，节点监控，服务发现等等。计算节点是k8s集群中真正工作的节点。

K8S架构细分： 

1、Master节点（默认不参加实际工作）：

`Kubectl`：客户端命令行工具，作为整个K8s集群的操作入口；

 `Api Server`：在K8s架构中承担的是“桥梁”的角色，作为资源操作的唯一入口，它提供了认证、授权、访问控制、API注册和发现等机制。客户端与k8s群集及K8s内部组件的通信，都要通过Api Server这个组件； 

`Controller-manager`：负责维护群集的状态，比如故障检测、自动扩展、滚动更新等；

 `Scheduler`：负责资源的调度，按照预定的调度策略将pod调度到相应的node节点上；

 `Etcd`：担任数据中心的角色，保存了整个群集的状态； 

2、Node节点：

 `Kubelet`：**负责维护容器的生命周期，同时也负责Volume和网络的管理**，一般运行在所有的节点，是Node节点的代理，当Scheduler确定某个node上运行pod之后，会将pod的具体信息（image，volume）等发送给该节点的kubelet，kubelet根据这些信息创建和运行容器，并向master返回运行状态。（自动修复功能：如果某个节点中的容器宕机，它会尝试重启该容器，若重启无效，则会将该pod杀死，然后重新创建一个容器）；

 `Kube-proxy`：Service在逻辑上代表了后端的多个pod。负责为Service提供cluster内部的**服务发现**和[负载均衡](https://cloud.tencent.com/product/clb?from=10680)（外界通过Service访问pod提供的服务时，Service接收到的请求后就是通过kube-proxy来转发到pod上的）； 

`container-runtime`：是负责管理运行容器的软件，比如docker Pod：是k8s集群里面最小的单位。每个pod里边可以运行一个或多个container（容器），如果一个pod中有两个container，那么container的USR（用户）、MNT（挂载点）、PID（进程号）是相互隔离的，UTS（主机名和域名）、IPC（消息队列）、NET（网络栈）是相互共享的。我比较喜欢把pod来当做豌豆夹，而豌豆就是pod中的container；

## 3、 容器和主机部署应用的区别是什么？

答：容器的中心思想就是秒级启动；一次封装、到处运行；这是主机部署应用无法达到的效果，但同时也更应该注重容器的数据持久化问题。 另外，容器部署可以将各个服务进行隔离，互不影响，这也是容器的另一个核心概念。

##  4、请你说一下kubenetes针对pod资源对象的健康监测机制？

答：K8s中对于pod资源对象的健康状态检测，提供了三类probe（探针）来执行对pod的健康监测：

1） livenessProbe探针 可以根据用户自定义规则来判定pod是否健康，如果livenessProbe探针探测到容器不健康，则kubelet会根据其重启策略来决定是否重启，如果一个容器不包含livenessProbe探针，则kubelet会认为容器的livenessProbe探针的返回值永远成功。

 2） ReadinessProbe探针 同样是可以根据用户自定义规则来判断pod是否健康，如果探测失败，控制器会将此pod从对应service的endpoint列表中移除，从此不再将任何请求调度到此Pod上，直到下次探测成功。

 3） startupProbe探针 启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被上面两类探针kill掉，这个问题也可以换另一种方式解决，就是定义上面两类探针机制时，初始化时间定义的长一些即可。

每种探测方法能支持以下几个相同的检查参数，用于设置控制检查时间：

initialDelaySeconds：初始第一次探测间隔，用于应用启动的时间，防止应用还没启动而健康检查失败 periodSeconds：检查间隔，多久执行probe检查，默认为10s； timeoutSeconds：检查超时时长，探测应用timeout后为失败； successThreshold：成功探测阈值，表示探测多少次为健康正常，默认探测1次。

上面两种探针都支持以下三种探测方法： 1）Exec：通过执行命令的方式来检查服务是否正常，比如使用cat命令查看pod中的某个重要配置文件是否存在，若存在，则表示pod健康。反之异常。 Exec探测方式的yaml文件语法如下：

spec: containers: – name: liveness image: k8s.gcr.io/busybox args: – /bin/sh – -c – touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: #选择livenessProbe的探测机制 exec: #执行以下命令 command: – cat – /tmp/healthy initialDelaySeconds: 5 #在容器运行五秒后开始探测 periodSeconds: 5 #每次探测的时间间隔为5秒

在上面的配置文件中，探测机制为在容器运行5秒后，每隔五秒探测一次，如果cat命令返回的值为“0”，则表示健康，如果为非0，则表示异常。

2）Httpget：通过发送http/htps请求检查服务是否正常，返回的状态码为200-399则表示容器健康（注http get类似于命令curl -I）。 Httpget探测方式的yaml文件语法如下：

spec: containers: – name: liveness image: k8s.gcr.io/liveness livenessProbe: #采用livenessProbe机制探测 httpGet: #采用httpget的方式 scheme:HTTP #指定协议，也支持https path: /healthz #检测是否可以访问到网页根目录下的healthz网页文件 port: 8080 #监听端口是8080 initialDelaySeconds: 3 #容器运行3秒后开始探测 periodSeconds: 3 #探测频率为3秒

上述配置文件中，探测方式为项容器发送HTTP GET请求，请求的是8080端口下的healthz文件，返回任何大于或等于200且小于400的状态码表示成功。任何其他代码表示异常。

3）tcpSocket：通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康，这种方式与HTTPget的探测机制有些类似，tcpsocket健康检查适用于TCP业务。 tcpSocket探测方式的yaml文件语法如下：

spec: containers: – name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: – containerPort: 8080 #这里两种探测机制都用上了，都是为了和容器的8080端口建立TCP连接 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20

在上述的yaml配置文件中，两类探针都使用了，在容器启动5秒后，kubelet将发送第一个readinessProbe探针，这将连接容器的8080端口，如果探测成功，则该pod为健康，十秒后，kubelet将进行第二次连接。

除了readinessProbe探针外，在容器启动15秒后，kubelet将发送第一个livenessProbe探针，仍然尝试连接容器的8080端口，如果连接失败，则重启容器。

探针探测的结果无外乎以下三者之一：

Success：Container通过了检查； Failure：Container没有通过检查； Unknown：没有执行检查，因此不采取任何措施（通常是我们没有定义探针检测，默认为成功）。

若觉得上面还不够透彻，可以移步其官网文档：https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ 5、 如何控制滚动更新过程？

答： 可以通过下面的命令查看到更新时可以控制的参数：

[root@master yaml]# kubectl explain deploy.spec.strategy.rollingUpdate

maxSurge ：此参数控制滚动更新过程，副本总数超过预期pod数量的上限。可以是百分比，也可以是具体的值。默认为1。 （上述参数的作用就是在更新过程中，值若为3，那么不管三七二一，先运行三个pod，用于替换旧的pod，以此类推） maxUnavailable：此参数控制滚动更新过程中，不可用的Pod的数量。 （这个值和上面的值没有任何关系，举个例子：我有十个pod，但是在更新的过程中，我允许这十个pod中最多有三个不可用，那么就将这个参数的值设置为3，在更新的过程中，只要不可用的pod数量小于或等于3，那么更新过程就不会停止）。 6、K8s中镜像的下载策略是什么？

答：可通过命令“kubectl explain pod.spec.containers”来查看imagePullPolicy这行的解释。

K8s的镜像下载策略有三种：Always、Never、IFNotPresent；

Always：镜像标签为latest时，总是从指定的仓库中获取镜像； Never：禁止从仓库中下载镜像，也就是说只能使用本地镜像； IfNotPresent：仅当本地没有对应镜像时，才从目标仓库中下载。 默认的镜像下载策略是：当镜像标签是latest时，默认策略是Always；当镜像标签是自定义时（也就是标签不是latest），那么默认策略是IfNotPresent。

## 7、 image的状态有哪些？

Running：Pod所需的容器已经被成功调度到某个节点，且已经成功运行， Pending：APIserver创建了pod资源对象，并且已经存入etcd中，但它尚未被调度完成或者仍然处于仓库中下载镜像的过程 Unknown：APIserver无法正常获取到pod对象的状态，通常是其无法与所在工作节点的kubelet通信所致。

## 8、 pod的重启策略是什么？

答：可以通过命令“kubectl explain pod.spec”查看pod的重启策略。（restartPolicy字段）

Always：但凡pod对象终止就重启，此为默认策略。 OnFailure：仅在pod对象出现错误时才重启

## 9、 Service这种资源对象的作用是什么？

答：用来给相同的多个pod对象提供一个固定的统一访问接口，常用于**服务发现和服务访问。**

##  10、版本回滚相关的命令？

```shell
[root@master httpd-web]# kubectl apply -f httpd2-deploy1.yaml –record #运行yaml文件，并记录版本信息； [root@master httpd-web]# kubectl rollout history deployment httpd-devploy1 #查看该deployment的历史版本 [root@master httpd-web]# kubectl rollout undo deployment httpd-devploy1 –to-revision=1 #执行回滚操作，指定回滚到版本1
```

\#在yaml文件的spec字段中，可以写以下选项（用于限制最多记录多少个历史版本）： spec: revisionHistoryLimit: 5 #这个字段通过 kubectl explain deploy.spec 命令找到revisionHistoryLimit 行获得

## 11、 标签与标签选择器的作用是什么？

标签：是当相同类型的资源对象越来越多的时候，为了更好的管理，可以按照标签将其分为一个组，为的是提升资源对象的管理效率。 标签选择器：就是标签的查询过滤条件。目前API支持两种标签选择器：

基于等值关系的，如：“=”、“”“==”、“！=”（注：“==”也是等于的意思，yaml文件中的matchLabels字段）； 基于集合的，如：in、notin、exists（yaml文件中的matchExpressions字段）； 注：in:在这个集合中；notin：不在这个集合中；exists：要么全在（exists）这个集合中，要么都不在（notexists）；

使用标签选择器的操作逻辑：

在使用基于集合的标签选择器同时指定多个选择器之间的逻辑关系为“与”操作（比如：- {key: name,operator: In,values: [zhangsan,lisi]} ，那么只要拥有这两个值的资源，都会被选中）； 使用空值的标签选择器，意味着每个资源对象都被选中（如：标签选择器的键是“A”，两个资源对象同时拥有A这个键，但是值不一样，这种情况下，如果使用空值的标签选择器，那么将同时选中这两个资源对象） 空的标签选择器（注意不是上面说的空值，而是空的，都没有定义键的名称），将无法选择出任何资源； 在基于集合的选择器中，使用“In”或者“Notin”操作时，其values可以为空，但是如果为空，这个标签选择器，就没有任何意义了。

两种标签选择器类型（基于等值、基于集合的书写方法）： selector: matchLabels: #基于等值 app: nginx matchExpressions: #基于集合 – {key: name,operator: In,values: [zhangsan,lisi]} #key、operator、values这三个字段是固定的 – {key: age,operator: Exists,values:} #如果指定为exists，那么values的值一定要为空

## 12、 常用的标签分类有哪些？

标签分类是可以自定义的，但是为了能使他人可以达到一目了然的效果，一般会使用以下一些分类：

版本类标签（release）：stable（稳定版）、canary（金丝雀版本，可以将其称之为测试版中的测试版）、beta（测试版）； 环境类标签（environment）：dev（开发）、qa（测试）、production（生产）、op（[运维](https://cloud.tencent.com/solution/operation?from=10680)）； 应用类（app）：ui、as、pc、sc； 架构类（tier）：frontend（前端）、backend（后端）、cache（缓存）； 分区标签（partition）：customerA（客户A）、customerB（客户B）； 品控级别（Track）：daily（每天）、weekly（每周）。

## 13、 有几种查看标签的方式？

答：常用的有以下三种查看方式：

[root@master ~]# kubectl get pod –show-labels #查看pod，并且显示标签内容 [root@master ~]# kubectl get pod -L env,tier #显示资源对象标签的值 [root@master ~]# kubectl get pod -l env,tier #只显示符合键值资源对象的pod，而“-L”是显示所有的pod

## 14、 添加、修改、删除标签的命令？

\#对pod标签的操作 [root@master ~]# kubectl label pod label-pod abc=123 #给名为label-pod的pod添加标签 [root@master ~]# kubectl label pod label-pod abc=456 –overwrite #修改名为label-pod的标签 [root@master ~]# kubectl label pod label-pod abc- #删除名为label-pod的标签 [root@master ~]# kubectl get pod –show-labels

\#对node节点的标签操作 [root@master ~]# kubectl label nodes node01 disk=ssd #给节点node01添加disk标签 [root@master ~]# kubectl label nodes node01 disk=sss –overwrite #修改节点node01的标签 [root@master ~]# kubectl label nodes node01 disk- #删除节点node01的disk标签

## 15、 DaemonSet资源对象的特性？

DaemonSet这种资源对象会在每个k8s集群中的节点上运行，并且每个节点只能运行一个pod，这是它和deployment资源对象的最大也是唯一的区别。所以，在其yaml文件中，不支持定义replicas，除此之外，与Deployment、RS等资源对象的写法相同。

它的一般使用场景如下：

在去做每个节点的日志收集工作； 监控每个节点的的运行状态；

## 16、 说说你对Job这种资源对象的了解？

答：Job与其他服务类容器不同，Job是一种工作类容器（一般用于做一次性任务）。使用常见不多，可以忽略这个问题。

\#提高Job执行效率的方法： spec: parallelism: 2 #一次运行2个 completions: 8 #最多运行8个 template: metadata:

## 17、描述一下pod的生命周期有哪些状态？

Pending：表示pod已经被同意创建，正在等待kube-scheduler选择合适的节点创建，一般是在准备镜像； Running：表示pod中所有的容器已经被创建，并且至少有一个容器正在运行或者是正在启动或者是正在重启； Succeeded：表示所有容器已经成功终止，并且不会再启动； Failed：表示pod中所有容器都是非0（不正常）状态退出； Unknown：表示无法读取Pod状态，通常是kube-controller-manager无法与Pod通信。

## 18、 创建一个pod的流程是什么？

答：

1） 客户端提交Pod的配置信息（可以是yaml文件定义好的信息）到kube-apiserver； 2） Apiserver收到指令后，通知给controller-manager创建一个资源对象； 3） Controller-manager通过api-server将pod的配置信息存储到ETCD数据中心中； 4） Kube-scheduler检测到pod信息会开始调度预选，会先过滤掉不符合Pod资源配置要求的节点，然后开始调度调优，主要是挑选出更适合运行pod的节点，然后将pod的资源配置单发送到node节点上的kubelet组件上。 5） Kubelet根据scheduler发来的资源配置单运行pod，运行成功后，将pod的运行信息返回给scheduler，scheduler将返回的pod运行状况的信息存储到etcd数据中心。

## 19、 删除一个Pod会发生什么事情？

答：Kube-apiserver会接受到用户的删除指令，默认有30秒时间等待优雅退出，超过30秒会被标记为死亡状态，此时Pod的状态Terminating，kubelet看到pod标记为Terminating就开始了关闭Pod的工作；

关闭流程如下： 1、 pod从service的endpoint列表中被移除； 2、 如果该pod定义了一个停止前的钩子，其会在pod内部被调用，停止钩子一般定义了如何优雅的结束进程； 3、 进程被发送TERM信号（kill -14） 4、 当超过优雅退出的时间后，Pod中的所有进程都会被发送SIGKILL信号（kill -9）。

## 20、 K8s的Service是什么？

答：Pod每次重启或者重新部署，其IP地址都会产生变化，这使得pod间通信和pod与外部通信变得困难，这时候，就需要Service为pod提供一个固定的入口。

Service的Endpoint列表通常绑定了一组相同配置的pod，通过负载均衡的方式把外界请求分配到多个pod上 

## 21、 k8s是怎么进行服务注册的？

答：Pod启动后会加载当前环境所有Service信息，以便不同Pod根据Service名进行通信。 

## 22、 k8s集群外流量怎么访问Pod？

答：可以通过Service的NodePort方式访问，会在所有节点监听同一个端口，比如：30000，访问节点的流量会被重定向到对应的Service上面。

##  23、 k8s数据持久化的方式有哪些？

### 1）EmptyDir（空目录）：

没有指定要挂载[宿主机](https://cloud.tencent.com/product/cdh?from=10680)上的某个目录，直接由Pod内保部映射到宿主机上。类似于docker中的manager volume。

主要使用场景： 1） 只需要临时将数据保存在磁盘上，比如在合并/排序算法中； 2） 作为两个容器的共享存储，使得第一个内容管理的容器可以将生成的数据存入其中，同时由同一个webserver容器对外提供这些页面。 emptyDir的特性： 同个pod里面的不同容器，共享同一个持久化目录，当pod节点删除时，volume的数据也会被删除。如果仅仅是容器被销毁，pod还在，则不会影响volume中的数据。 总结来说：emptyDir的数据持久化的生命周期和使用的pod一致。一般是作为临时存储使用。

### 2）Hostpath：

将宿主机上已存在的目录或文件挂载到容器内部。类似于docker中的bind mount挂载方式。 这种数据持久化方式，运用场景不多，因为它增加了pod与节点之间的耦合。 一般对于k8s集群本身的数据持久化和docker本身的数据持久化会使用这种方式，可以自行参考apiService的yaml文件，位于：/etc/kubernetes/main…目录下。

### 3）PersistentVolume（简称PV）：

 基于NFS服务的PV，也可以基于GFS的PV。它的作用是统一数据持久化目录，方便管理。



在一个PV的yaml文件中，可以对其配置PV的大小， 指定PV的访问模式：

ReadWriteOnce：只能以读写的方式挂载到单个节点； ReadOnlyMany：能以只读的方式挂载到多个节点； ReadWriteMany：能以读写的方式挂载到多个节点。， 以及指定pv的回收策略： recycle：清除PV的数据，然后自动回收； Retain：需要手动回收； delete：删除云存储资源，云存储专用； #PS：这里的回收策略指的是在PV被删除后，在这个PV下所存储的源文件是否删除）。

若需使用PV，那么还有一个重要的概念：PVC，PVC是向PV申请应用所需的容量大小，K8s集群中可能会有多个PV，PVC和PV若要关联，其定义的访问模式必须一致。定义的storageClassName也必须一致，若群集中存在相同的（名字、访问模式都一致）两个PV，那么PVC会选择向它所需容量接近的PV去申请，或者随机申请。









## **1、什么是k8s？**

```text
 Kubenetes是一个针对容器应用，进行自动部署，弹性伸缩和管理的开源系统。主要功能是生产环境中的容器编排。

 K8S是Google公司推出的，它来源于由Google公司内部使用了15年的Borg系统，集结了Borg的精华。
 
k8s是一个docker集群的管理工具 
k8s是容器的编排工具
```

## **2、k8s的核心功能**

### **1.自愈**

```text
自愈:  重新启动失败的容器，在节点不可用时，替换和重新调度节点上的容器，对用户定义的健康检查不响应的容器会被中止，并且在容器准备好服务之前不会把其向客户端广播。
```

### **2.弹性伸缩**

```text
通过监控容器的cpu的负载值,如果这个平均高于80%,增加容器的数量,如果这个平均低于10%,减少容器的数量
```

### **3.服务的自动发现和负载均衡**

```text
不需要修改您的应用程序来使用不熟悉的服务发现机制，Kubernetes 为容器提供了自己的 IP 地址和一组容器的单个 DNS 名称，并可以在它们之间进行负载均衡。
```

### **4.滚动升级和一键回滚**

```text
Kubernetes 逐渐部署对应用程序或其配置的更改，同时监视应用程序运行状况，以确保它不会同时终止所有实例。 如果出现问题，Kubernetes会为您恢复更改，利用日益增长的部署解决方案的生态系统。
```

### **5.私密配置文件管理**

```text
web容器里面,数据库的账户密码(测试库密码)
```

## **3、k8s的组成？**

![img](https://pic2.zhimg.com/80/v2-9e31530c209df652faa4d41fc2a7e255_720w.jpg)

### **1.Master节点（默认不参加工作）**

```text
Kubectl：
 客户端命令行工具，作为整个K8s集群的操作入口；

Api Server：
 在K8s架构中承担的是“桥梁”的角色，作为资源操作的唯一入口，它提供了认证、授权、访问控制、API注册和发现等机制。客户端与k8s群集及K8s内部组件的通信，都要通过Api Server这个组件；

Controller-manager：
 负责维护群集的状态，比如故障检测、自动扩展、滚动更新等

Scheduler：
 负责资源的调度，按照预定的调度策略将pod调度到相应的node节点上；
```

> Etcd(可以不在master节点)：担任数据中心的角色，保存了整个群集的状态；

### **2.Node节点**

```text
Kubelet：
 负责维护容器的生命周期，同时也负责Volume和网络的管理，一般运行在所有的节点，是Node节点的代理，当Scheduler确定某个node上运行pod之后，会将pod的具体信息（image，volume）等发送给该节点的kubelet，kubelet根据这些信息创建和运行容器，并向master返回运行状态。（自动修复功能：如果某个节点中的容器宕机，它会尝试重启该容器，若重启无效，则会将该pod杀死，然后重新创建一个容器）；

Kube-proxy：
 Service在逻辑上代表了后端的多个pod。负责为Service提供cluster内部的服务发现和负载均衡（外界通过Service访问pod提供的服务时，Service接收到的请求后就是通过kube-proxy来转发到pod上的）；
 
container-runtime：
 是负责管理运行容器的软件，比如docker
```

### **3.扩展组件**

| 组件名称 | 说明 |
| -------- | ---- |
|          |      |

## **4、说出对fannel的了解**

![img](https://pic1.zhimg.com/80/v2-dac4bf5e51d01915a557720475d89bdc_720w.jpg)

### **1.功能**

```text
 让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。但在默认的Docker配置中，每个Node的Docker服务会分别负责所在节点容器的IP分配。Node内部得容器之间可以相互访问,但是跨主机(Node)网络相互间是不能通信。Flannel设计目的就是为集群中所有节点重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得"同属一个内网"且"不重复的"IP地址，并让属于不同节点上的容器能够直接通过内网IP通信。
```

### **2.原理**

```text
 Flannel 使用etcd存储配置数据和子网分配信息。flannel 启动之后，后台进程首先检索配置和正在使用的子网列表，然后选择一个可用的子网，然后尝试去注册它。etcd也存储这个每个主机对应的ip。flannel 使用etcd的watch机制监视/coreos.com/network/subnets下面所有元素的变化信息，并且根据它来维护一个路由表。为了提高性能，flannel优化了Universal TAP/TUN设备，对TUN和UDP之间的ip分片做了代理。
```

### **3.工作流程**

```text
 1、数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，这是个P2P的虚拟网卡，flanneld服务监听在网卡的另外一端。
 
 2、Flannel通过Etcd服务维护了一张节点间的路由表，该张表里保存了各个节点主机的子网网段信息。

 3、源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一样的由docker0路由到达目标容器。
```

## **5、说一下你对fannel和calico了解及区别？**

**Flannel**

```text
 由CoreOS开发的项目Flannel，可能是最直接和最受欢迎的CNI插件。它是容器编排系统中最成熟的网络结构示例之一，旨在实现更好的容器间和主机间网络。随着CNI概念的兴起，Flannel CNI插件算是早期的入门。
```

1）安装简单，不需要专门的数据存储

```text
 Flannel相对容易安装和配置。它被打包为单个二进制文件flanneld，许多常见的Kubernetes集群部署工具和许多Kubernetes发行版都可以默认安装Flannel。Flannel可以使用Kubernetes集群的现有etcd集群来使用API存储其状态信息，因此不需要专用的数据存储。

 Flannel配置第3层IPv4 overlay网络。它会创建一个大型内部网络，跨越集群中每个节点。在此overlay网络中，每个节点都有一个子网，用于在内部分配IP地址。在配置pod时，每个节点上的Docker桥接口都会为每个新容器分配一个地址。同一主机中的Pod可以使用Docker桥接进行通信，而不同主机上的pod会使用flanneld将其流量封装在UDP数据包中，以便路由到适当的目标。

 Flannel有几种不同类型的后端可用于封装和路由。默认和推荐的方法是使用VXLAN，因为VXLAN性能更良好并且需要的手动干预更少。

 总的来说，Flannel是大多数用户的不错选择。从管理角度来看，它提供了一个简单的网络模型，用户只需要一些基础知识，就可以设置适合大多数用例的环境。一般来说，在初期使用Flannel是一个稳妥安全的选择，直到你开始需要一些它无法提供的东西。
```

**Calico**

1）功能全面、灵活性高。

```text
 Calico是Kubernetes生态系统中另一种流行的网络选择。虽然Flannel被公认为是最简单的选择，但Calico以其性能、灵活性而闻名。Calico的功能更为全面，不仅提供主机和pod之间的网络连接，还涉及网络安全和管理。Calico CNI插件在CNI框架内封装了Calico的功能。
```

2）不需要额外的NAT、隧道或者Overlay Network，没有额外的封包解包，能够节约CPU运算，提高网络效率。

```text
 它创建的网络环境具有简单和复杂的属性。与Flannel不同，Calico不使用overlay网络。相反，Calico配置第3层网络，该网络使用BGP路由协议在主机之间路由数据包。这意味着在主机之间移动时，不需要将数据包包装在额外的封装层中。BGP路由机制可以本地引导数据包，而无需额外在流量层中打包流量。
```

3）有标准的调试工具，方便排错

```text
除了性能优势之外，在出现网络问题时，用户还可以用更常规的方法进行故障排除。虽然使用VXLAN等技术进行封装也是一个不错的解决方案，但该过程处理数据包的方式同场难以追踪。使用Calico，标准调试工具可以访问与简单环境中相同的信息，从而使更多开发人员和管理员更容易理解行为。
```

**总结**

```text
 两者都属于k8s网络插件。我们公司使用的是flannel, calico性能更好也更灵活，但是flannel更加简单好用，当公司由二次开发kubernetes的场景时，建议使用calico，当有IPV6使用场景时，必须使用calico。
```

## **6、kubectl这个命令执行的过程？(以部署nginx服务为例)**

```text
1、kubectl发送了一个部署nginx的任务
2、进入Master节点，进行安全认证，
3、认证通过后，APIserver接受指令
4、将部署的命令数据记录到etcd中
5、APIserver再读取etcd中的命令数据
6、APIserver找到scheduler（调度器），说要部署nginx
7、scheduler（调度器）找APIserver调取工作节点数据。
8、APIserver调取etcd中存储的数据，并将数据发给scheduler。
9、scheduler通过计算，比较找到适合部署nginx的最佳节点是node1，发送给APIserver。
10、APIserver将要部署在node1的计划存储到etcd中。
11、APIserver读取etcd中的部署计划，通知node1节点的kubelet部署容器
12、kubelet根据指令部署nginx容器，kube-proxy为nginx容器创建网桥
13、容器网桥部署完成后，kubelet通知APIserver部署工作完成。
14、APIserver将部署状态存储到etcd当中，同时通知controller manager(控制器）有新活了
15、controller manager向APIserver要需要监控容器的数据
16、APIserver找etcd读取相应数据，同时通知kubelet要源源不断发送监控的数据
17、APIserver将kubelet发送来的数据存储到etcd当中
18、APIserver将etcd的数据返回给controller manager
19、controller manager根据数据计算判断容器是否存在或健康
```

## **7、容器和主机部署应用的区别是什么？**

```text
 容器的中心思想就是秒级启动；一次封装、到处运行；这是主机部署应用无法达到的效果，但同时也更应该注重容器的数据持久化问题。

 另外，容器部署可以将各个服务进行隔离，互不影响，这也是容器的另一个核心概念。
```

## **8、说一下pod的生命周期？**

![img](https://pic2.zhimg.com/80/v2-156c7d6de048c5366afdd57e92c78871_720w.jpg)

```text
1、启动包括初始化容器的任何容器之前先创建pause基础容器，它初始化Pod环境并为后续加入的容器提供共享的名称空间。
2、按顺序以串行的方式运行用户定义的各个初始化容器进行Pod环境初始化；任何一个初始化容器运行失败都将导致Pod创建失败，并按其restartPolicy的策略进行处理，默认为重启。
3、等待所有容器初始化成功完成后，启动业务容器，多容器Pod环境中，此步骤会并行启动所有业务容器。他们各自按其自定义展开其生命周期；容器启动的那一刻会同时运行业务容器上定义的PostStart钩子事件，该步骤失败将导致相关容器被重启。
4、运行容器启动健康状态监测（startupProbe），判断容器是否启动成功；该步骤失败，同样参照restartPolicy定义的策略进行处理；未定义时，默认状态为Success。
5、容器启动成功后，定期进行存活状态监测（liveness）和就绪状态监测（readiness）；存活监测状态失败将导致容器重启，而就绪状态监测失败会是的该容器从其所属的Service对象的可用端点列表中移除。
6、终止Pod对象时，会想运行preStop钩子事件，并在宽限期（termiunationGracePeriodSeconds）结束后终止主容器，宽限期默认为30秒。

#简述
1、创建pod，并调度到合适节点
2、创建pause基础容器，提供共享名称空间
3、串行业务容器容器初始化
4、启动业务容器，启动那一刻会同时运行主容器上定义的Poststart钩子事件
5、健康状态监测，判断容器是否启动成功
6、持续存活状态监测、就绪状态监测
7、结束时，执行prestop钩子事件
8、终止容器
```

## **9、描述一下pod的生命周期有哪些状态？**

```text
Pending：表示pod已经被同意创建，正在等待kube-scheduler选择合适的节点创建，一般是在准备镜像；

Running：表示pod中所有的容器已经被创建，并且至少有一个容器正在运行或者是正在启动或者是正在重启；

Succeeded：表示所有容器已经成功终止，并且不会再启动；

Failed：表示pod中所有容器都是非0（不正常）状态退出；

Unknown：表示无法读取Pod状态，通常是kube-controller-manager无法与Pod通信。
```

## **10、说一下PostStart、PreStop钩子？**

```text
 PostStart :在容器创建后立即执行。但是，并不能保证钩子将在容器ENTRYPOINT之前运行，因为没有参数传递给处理程序。 主要用于资源部署、环境准备等。不过需要注意的是如果钩子花费时间过长以及于不能运行或者挂起，容器将不能达到Running状态。
 容器启动后执行，注意由于是异步执行，它无法保证一定在ENTRYPOINT之后运行。如果失败，容器会被杀死，并根据RestartPolicy决定是否重启
 
 PreStop :在容器终止前立即被调用。它是阻塞的，意味着它是同步的，所以它必须在删除容器的调用出发之前完成。主要用于优雅关闭应用程序、通知其他系统等。如果钩子在执行期间挂起，Pod阶段将停留在Running状态并且不会达到failed状态
 容器停止前执行，常用于资源清理。如果失败，容器同样也会被杀死
```

## **11、请你说一下kubenetes针对pod资源对象的健康监测机制，以及三种检查方式？**

### **1.livenessProbe探针(检查是否存活)**

```text
可以根据用户自定义规则来判定pod是否健康，如果livenessProbe探针探测到容器不健康，则kubelet会根据其重启策略来决定是否重启，如果一个容器不包含livenessProbe探针，则kubelet会认为容器的livenessProbe探针的返回值永远成功。
```

### **2.ReadinessProbe探针（检查是否就绪）**

[rml_read_more]：

```text
同样是可以根据用户自定义规则来判断pod是否健康，如果探测失败，控制器会将此pod从对应service的endpoint列表中移除，从此不再将任何请求调度到此Pod上，直到下次探测成功。
```

### **3.startupProbe探针（检查是否启动成功）**

```text
启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被上面两类探针kill掉，这个问题也可以换另一种方式解决，就是定义上面两类探针机制时，初始化时间定义的长一些即可。
```

### **4.检查方式?**

### **1)exec:**

```text
通过执行命令的方式来检查服务是否正常，比如使用cat命令查看pod中的某个重要配置文件是否存在，若存在，则表示pod健康。反之异常。
```

### **2)Httpget：**

```text
通过发送http/htps请求检查服务是否正常，返回的状态码为200-399则表示容器健康（注http get类似于命令curl -I）。
```

### **3)tcpSocket：**

```text
通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康，这种方式与HTTPget的探测机制有些类似，tcpsocket健康检查适用于TCP业务。
```

## **12、K8s中镜像的下载策略是什么？**

```text
1、可通过命令“kubectl explain pod.spec.containers”来查看imagePullPolicy这行的解释。

K8s的镜像下载策略有三种：Always、Never、IFNotPresent；

Always：镜像标签为latest时，总是从指定的仓库中获取镜像；

Never：禁止从仓库中下载镜像，也就是说只能使用本地镜像；

IfNotPresent：仅当本地没有对应镜像时，才从目标仓库中下载。

默认的镜像下载策略是：当镜像标签是latest时，默认策略是Always；当镜像标签是自定义时（也就是标签不是latest），那么默认策略是IfNotPresent。
```

## **13、pod的重启策略是什么？**

```text
 Pod 重启策略（ RestartPolicy ）应用于 Pod 内的所有容器，井且仅在 Pod 所处的 Node 上由 kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时， kubelet 将根据 RestartPolicy 设置来进行相应的操作。Pod 的重启策略包括：Always、OnFailure 和 Never，默认值为 Always
```

> Always：当容器失效时，由kubelet自动启动该容器
> OnFailure：当容器终止运行且退出代码不为0时，有kubelet自动重启该容器
> Never：不论容器运行状态如何，kubelet都不会重启该容器

```text
 kubelet重启失效容器的时间间隔以sync-frequency乘以2n来计算；例如1、2、4、8倍等，最长延时5min，并且在成功重启后的10min后重置该时间。
 Pod的重启策略与控制方式息息相关，当前可用于管理Pod的控制器包括ReplicationController、Job、DaemonSet及直接通过kubelet管理（静态Pod）。每种控制器对Pod的重启策略要求如下：
```

> 1．RC和DaemonSet：必须设置为Always，需要保证该容器持续运行
> 2．Job和CronJob：OnFailure或Never，确保容器执行完成后不再重启。
> 3．kubelet：在Pod失效时自动重启它，不论将RestartPolicy设置为什么值，也不会对Pod进行健康检查

## **14、标签和标签选择器的作用是什么？**

### **1.标签**

```text
 是当相同类型的资源对象越来越多的时候，为了更好的管理，可以按照标签将其分为一个组，为的是提升资源对象的管理效率。
```

### **2.标签选择器**

```text
就是标签的查询过滤条件。目前API支持两种标签选择器：
```

### **1）基于等值关系**

```text
 如：“=”、“”“”、“！=”（注：“”也是等于的意思，yaml文件中的matchLabels字段）；
```

### **2）基于集合的**

```text
 基于集合的，如：in、notin、exists（yaml文件中的matchExpressions字段）；
```

注：in:在这个集合中；notin：不在这个集合中；exists：要么全在（exists）这个集合中，要么都不在（notexists）；

## **15、查看标签的方式**

```text
kubectl get node --show-labels

kubectl get pod --show-labels
```

## **16、添加、修改、删除标签的命令？**

```text
kubectl label node node-1 app=nginx

kubectl label node node-1 app=mycat --overwrite

kubectl label node node-1 app-
```

## **17、删除一个pod集群内部会发生什么？**

```text
kube-apiserver会接受到用户的删除指令，默认有30秒时间等待优雅退出，超过30秒会被标记为死亡状态

此时Pod的状态是Terminating，Kubelet看到Pod标记为Terminating开始了关闭Pod的工作

1、Pod从service的列表中被删除

2、如果该Pod定义了一个停止前的钩子，其会在pod内部被调用，停止钩子一般定义了如何优雅结束进程

3、进程被发送TERM信号（kill -14）

4、当超过优雅退出时间时，Pod中的所有进程都很被发送SIGKILL信号（kill -9）
```

## **18、持久化的方式有哪些？**

```text
EmptyDir（空目录）：没有指定要挂载宿主机上的某个目录，直接由Pod内保部映射到宿主机上。类似于docker中的manager volume。

Hostpath：将宿主机上已存在的目录或文件挂载到容器内部。类似于docker中的bind mount挂载方式。这种数据持久化方式，运用场景不多，因为它增加了pod与节点之间的耦合。

PersistentVolume（简称PV）：基于NFS服务的PV，也可以基于GFS的PV。它的作用是统一数据持久化目录，方便管理。
```

## **19、kubernetes 认证**

```text
 kubernetes 提供了多种认证方式，比如客户端证书,静态token,静态密码文件,ServiceAccountTokens等等。你可以同时使用一种或多种认证方式。只要通过任何一个都被认作是认证通过。
```

## **20、kube-apiserver和kube-scheduler的作用是什么?**

```text
kube -apiserver
 遵循横向扩展架构，是主节点控制面板的前端。这将公开Kubernetes主节点组件的所有API，并负责在Kubernetes节点和Kubernetes主组件之间建立通信。

kube-scheduler
 负责工作节点上工作负载的分配和管理。因此，它根据资源需求选择最合适的节点来运行未调度的pod，并跟踪资源利用率。它确保不在已满的节点上调度工作负载。
```

## **21、kube-scheduler工作原理，多少节点对外提供服务？**

```text
根据各种调度算法将 Pod 绑定到最合适的工作节点

预选（Predicates）：
 输入是所有节点，输出是满足预选条件的节点。kube-scheduler根据预选策略过滤掉不满足策略的Nodes。例如，如果某节点的资源不足或者不满足预选策略的条件如“Node的label必须与Pod的Selector一致”时则无法通过预选。

优选（Priorities）：
 输入是预选阶段筛选出的节点，优选会根据优先策略为通过预选的Nodes进行打分排名，选择得分最高的Node。例如，资源越富裕、负载越小的Node可能具有越高的排名。
```

## **22、集群使用的网络方案，pod如何和node网络通信的？**

```text
Flannel：使用vxlan技术为各节点创建一个可以互通的Pod网络，使用的端口为UDP 8472（需要开放该端口，如公有云AWS等）。
flanneld第一次启动时，从etcd获取配置的Pod网段信息，为本节点分配一个未使用的地址段，然后创建flannedl.1网络接口（也可能是其它名称，如flannel1等）。
flannel将分配给自己的Pod网段信息写入 /run/flannel/subnet.env文件，docker后续使用这个文件中的环境变量设置docker0网桥，从而从这个地址段为本节点的所有Pod容器分配IP。
```

## **23、k8s集群节点需要关机维护，需要怎么操作？**

```text
# 驱逐 node 节点上 pod
$ kubectl drain k8s-node-01 --force --ignore-daemonsets
# 关机$ init 0
```

## **24、生产中碰到过什么问题，故障排查思路，如何解决的？**

### **1.故障归类**

```text
Pod状态 一直处于 Pending

Pod状态 一直处于 Waiting

Pod状态 一直处于 ContainerCreating

Pod状态 处于 ImagePullBackOff

Pod状态 处于 CrashLoopBackOff

Pod状态 处于 Error

Pod状态 一直处于 Terminating

Pod状态 处于 Unknown
```

### **2.排查故障的命令**

1）查看Pod配置是否正确

```text
kubectl get pod <pod-name> -o yaml
```

2）查看Pod详细信息

```text
kubectl describe pod <pod-name>
```

3）查看容器日志

```text
kubectl logs <pod-name> [-c <container-name>]
```

### **3.故障问题与排查方法**

### **1）Pod 一直处于 Pending状态**

```text
Pending状态
 这个状态意味着，Pod 的 YAML 文件已经提交给 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功（可以通过 kubectl describe pod命令查看到当前 Pod 的事件，进而判断为什么没有调度）。
```

**可能原因**

```text
 资源不足（集群内所有的 Node 都不满足该 Pod 请求的 CPU、内存、GPU 等资源）；HostPort 已被占用（通常推荐使用 Service 对外开放服务端口）。
```

### **2）Pod 一直处于 Waiting或 ContainerCreating状态**

> 通过 kubectl describe pod命令查看到当前Pod的事件。

**可能原因**

```text
1、镜像拉取失败比如，镜像地址配置错误、拉取不了国外镜像源（gcr.io）、私有镜像密钥配置错误、镜像太大导致拉取超时（可以适当调整 kubelet 的 --image-pull-progress-deadline 和 --runtime-request-timeout 选项）等。

2、CNI 网络错误，一般需要检查 CNI 网络插件的配置，比如：无法配置 Pod 网络、无法分配 IP 地址。

3、容器无法启动，需要检查是否打包了正确的镜像或者是否配置了正确的容器参数。

4、Failed create pod sandbox，查看kubelet日志，原因可能是磁盘坏道（input/output error）。
```

### **3）Pod 一直处于 ImagePullBackOff状态**

> 通常是镜像名称配置错误或者私有镜像的密钥配置错误导致。这种情况可以使用 docker pull来验证镜像是否可以正常拉取。

如果私有镜像密钥配置错误或者没有配置，按下面检查：

### **①查询 docker-registry 类型的 Secret**

```text
# 查看 docker-registry Secret 
$ kubectl  get secrets my-secret -o yaml | grep 'dockerconfigjson:' | awk '{print $NF}' | base64 -d
```

### **②创建 docker-registry 类型的 Secret**

```text
# 首先创建一个 docker-registry 类型的 Secret
$ kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL

# 然后在 Deployment 中引用这个 Secret
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: my-secret
```

### **4）Pod 一直处于 CrashLoopBackOff 状态**

> CrashLoopBackOff 状态说明容器曾经启动了，但又异常退出。此时可以先查看一下容器的日志。

```text
通过命令 kubectl logs 和 kubectl logs --previous 可以发现一些容器退出的原因，比如：容器进程退出、健康检查失败退出、此时如果还未发现线索，还可以到容器内执行命令来进一步查看退出原因（kubectl exec cassandra – cat /var/log/cassandra/system.log），如果还是没有线索，那就需要 exec 登录该 Pod 所在的 Node 上，查看 Kubelet 或者 Docker 的日志进一步排查。
```

### **5）Pod 处于 Error 状态**

> 通常处于 Error 状态说明 Pod 启动过程中发生了错误

**常见原因**

```text
 依赖的 ConfigMap、Secret 或者 PV 等不存在；请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等；违反集群的安全策略，比如违反了 PodSecurityPolicy 等；容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定;
```

### **6）Pod 处于 Terminating 或 Unknown 状态**

> 从 v1.5 开始，Kubernetes 不会因为 Node 失联而删除其上正在运行的 Pod，而是将其标记为 Terminating 或 Unknown 状态。想要删除这些状态的 Pod 有三种方法：

```text
1、从集群中删除该 Node。使用公有云时，kube-controller-manager 会在 VM 删除后自动删除对应的 Node。而在物理机部署的集群中，需要管理员手动删除 Node（如 kubectl delete node ）。

2、Node 恢复正常。Kubelet 会重新跟 kube-apiserver 通信确认这些 Pod 的期待状态，进而再决定删除或者继续运行这些 Pod。用户强制删除。用户可以执行 kubectl delete pods pod-name --grace-period=0 --force 强制删除 Pod。除非明确知道 Pod 的确处于停止状态（比如 Node 所在 VM 或物理机已经关机），否则不建议使用该方法。特别是 StatefulSet 管理的 Pod，强制删除容易导致脑裂或者数据丢失等问题。

3、Pod 行为异常，这里所说的行为异常是指 Pod 没有按预期的行为执行，比如没有运行 podSpec 里面设置的命令行参数。这一般是 podSpec yaml 文件内容有误，可以尝试使用 --validate 参数重建容器，比如:

 kubectl delete pod mypod 和 kubectl create --validate -f mypod.yaml，也可以查看创建后的 podSpec 是否是对的，比如：kubectl get pod mypod -o yaml，修改静态 Pod 的 Manifest 后未自动重建，Kubelet 使用 inotify 机制检测 /etc/kubernetes/manifests 目录（可通过 Kubelet 的 --pod-manifest-path 选项指定）中静态 Pod 的变化，并在文件发生变化后重新创建相应的 Pod。但有时也会发生修改静态 Pod 的 Manifest 后未自动创建新 Pod 的情景，此时一个简单的修复方法是重启 Kubelet。

 Unknown 这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。
```

## **25、 K8S中request和limit区别？**

### **requst 资源请求量**

**含义**

```text
容器运行时，向k8s节点申请的最少保障资源
```

**cpu的request**

```text
cpu的request、limit会反映在容器的cgroup参数上
```

**内存的request**

```text
内存的request不会反映在容器的cgroup参数上，但limit会。
	所以容器内存即使有request，但是在容器的cgroup不被采用作为限制，那么其他没有limit或limit比request大的容器，就会来抢占这里的内存，导致这里的内存不足，结果是k8s节点并未保障容器的内存request，request - current的内存被其他容器占用
```

**调度规则**

```text
k8s节点的request剩余总资源大于容器请求的request资源，容器才能被分配到该节点，否则不予调度
```

### **limit 资源上限**

```text
含义：容器在k8s节点上消耗的资源上限
```

## **26、你对K8S控制器了解过哪些？**

### **1.Deployment**

```text
 在Deployment对象中描述所需的状态，然后Deployment控制器将实际状态以受控的速率更改为所需的状态。
 部署无状态应用
 
特点：集群之中，随机部署
```

### **2.DaemonSet**

```text
每一个节点上部署一个Pod，删除节点自动删除对应的POD（zabbix-agent）

特点：每一台上有且只有一台

使用场景：

  运行集群的存储daemon,

  每个Node的日志收集, fluentd ,logtash

  每个Node的监控程序.
```

### **3.Job**

```text
负责批量任务，仅执行一次任务， 保证批处理任务一个或多个Pod成功结束
```

### **4.CronJob**

```text
定时调度某个时间执行一次或循环多次执行
```

### **5.StatefulSet**

```text
作为Controller为Pod提供唯一的标识 , 证部署和scale的顺序。StatefulSet用来解决有状态服务的问题，

场景：
 1、稳定的持久化存储, 即Pod重新调度后能访问持久化数据，基于PVC实现,

 2、稳定的网络标识, 即Pod重新调度后PodName和HostName不变, 基于Headless Services(即没有Cluster IP的Service) 来实现.

 3、有序部署和有序扩展， 0-> N  下一个执行基于前一个已经Running或Ready, 基于InitC实现

 4、有序删除N->0
```

## **27、你们K8S监控怎么做的？**

使用prometheus

### **1.Kubernetes监控什么？**

### **1）node节点**

### **①Node资源利用率**

```text
 一般生产环境几十个node，几百个node去监控
```

### **②Node数量**

```text
 一般能监控到node，就能监控到它的数量了，因为它是一个实例，一个node能跑多少个项目，也是需要去评估的，整体资源率在一个什么样的状态，什么样的值，所以需要根据项目，跑的资源利用率，还有值做一个评估的，比如再跑一个项目，需要多少资源。
```

### **③Pods数量（Node）**

```text
 其实也是一样的，每个node上都跑多少pod,不过默认一个node上能跑110个pod，但大多数情况下不可能跑这么多，比如一个128G的内存，32核cpu,一个java的项目，一个分配2G,也就是能跑50-60个，一般机器，pod也就跑几十个，很少很少超过100个。
```

### **④资源对象状态**

```text
比如pod，service,deployment,job这些资源状态，做一个统计。
```

### **2）Pod**

### **①Pod数量（项目）**

```text
 你的项目跑了多少个pod的数量，大概的利益率是多少，好评估一下这个项目跑了多少个资源占有多少资源，每个pod占了多少资源。
```

### **②容器资源使用率**

```text
每个容器消耗了多少资源，用了多少CPU，用了多少内存
```

### **③应用程序**

```text
 这个就是偏应用程序本身的指标了，这个一般在我们运维很难拿到的，所以在监控之前呢，需要开发去给你暴露出来，这里有很多客户端的集成，客户端库就是支持很多语言的，需要让开发做一些开发量将它集成进去，暴露这个应用程序的想知道的指标，然后纳入监控，如果开发部配合，基本运维很难做到这一块，除非自己写一个客户端程序，通过shell/python能不能从外部获取内部的工作情况，如果这个程序提供API的话，这个很容易做到。
```

![img](https://pic3.zhimg.com/80/v2-bbced54a395d2c14288350eccce01912_720w.jpg)

### **2.怎么监控？**

### **1）node节点**

```text
 如果想监控node的资源，就可以放一个node_exporter,这是监控node资源的，node_exporter是Linux上的采集器，你放上去你就能采集到当前节点的CPU、内存、网络IO，等待都可以采集的。
```

### **2）Pod容器**

```text
 如果想监控容器，k8s内部提供cAdvisor采集器，pod呀，容器都可以采集到这些指标，都是内置的，不需要单独部署，只知道怎么去访问这个Cadvisor就可以了。
```

### **3）k8s其他资源对象**

```text
 如果想监控k8s资源对象，会部署一个kube-state-metrics这个服务，它会定时的API中获取到这些指标，帮你存取到Prometheus里，要是告警的话，通过Alertmanager发送给一些接收方，通过Grafana可视化展示。
```

| 监控指标 | 具体实现 | 举例 |
| -------- | -------- | ---- |
|          |          |      |

**服务发现：[https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config](https://link.zhihu.com/?target=https%3A//prometheus.io/docs/prometheus/latest/configuration/configuration/%23kubernetes_sd_config)**

### **4）应用程序**

> 分为四种情况

```text
1、容器化普罗米修斯携带metrics
2、容器化普罗米修斯无metrics
3、物理机普罗米修斯携带metrics
4、物理机普罗米修斯无metrics
```

**容器化普罗米修斯**

### **①携带metrics**

```text
1、创建一个EndProints（将外部的应用的metrics接口的地址接入集群）
2、创建跟上条的EndPrints同名称的Service
 1、只有创建service跟endproints同名称才能够关联上
 2、将endPrints接入的地址，通过service交给集群使用
3、部署ServiceMonitor
 1、将上述接入集群的service中的需要被监控的metrics接口注入到普罗米修斯
```

### **②无metrics**

```text
1、创建一个EndProints（将外部的应用的metrics接口的地址接入集群）
2、创建跟上条的EndPrints同名称的Service
 1、只有创建service跟endproints同名称才能够关联上
 2、将endPrints接入的地址，通过service交给集群使用
3、部署exporter,构建出一个metrics接口（1和2步合并进来）
4、部署ServiceMonitor
 1、将上述接入集群的service中的需要被监控的metrics接口注入到普罗米修斯
```

**物理机普罗米修斯**

### **①携带metrics**

```text
1、将metrics接口地址写入普罗米修斯配置文件
2、重启普罗米修斯
```

### **②无metrics**

```text
1、部署exporter,构建出一个metrics接口
2、将metrics接口地址写入普罗米修斯配置文件
3、重启普罗米修斯
```

## **28、k8s的日志收集怎么做的？**

### **1.日志收集级别**

```text
应用(Pod)级别
节点级别
集群级别
```

### **1）应用（Pod级别）**

```text
Pod级别的日志 , 默认是输出到标准输出和标志输入，实际上跟docker 容器的一致。使用 kubectl logs pod-name -n namespace 查看
```

[https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs](https://link.zhihu.com/?target=https%3A//kubernetes.io/docs/reference/generated/kubectl/kubectl-commands%23logs)

### **2）节点级别**

```text
 Node级别的日志 , 通过配置容器的log-driver来进行管理 , 这种需要配合logrotare来进行 , 日志超过最大限制 , 自动进行logrotate操作。
```

log-driver:

[https://docs.docker.com/config/containers/logging/configure/](https://link.zhihu.com/?target=https%3A//docs.docker.com/config/containers/logging/configure/)

logrotate

[https://linux.die.net/man/8/logrotate](https://link.zhihu.com/?target=https%3A//linux.die.net/man/8/logrotate)

![img](https://pic3.zhimg.com/80/v2-43407d59b6b22f686d9398650b31a37a_720w.jpg)

### **3）集群级别（三种）**

### **①节点代理**

```text
 在node级别进行日志收集。一般使用DaemonSet部署在每个node中。这种方式优点是耗费资源少，因为只需部署在节点，且对应用无侵入。缺点是只适合容器内应用日志必须都是标准输出。
```

![img](https://pic3.zhimg.com/80/v2-b00f7ed20bc687c678c49cb8f7f81a56_720w.jpg)

### **②使用sidecar container作为容器日志代理**

> 也就是在pod中跟随应用容器起一个日志处理容器，有两种形式：

1）

```text
 直接将应用容器的日志收集并输出到标准输出（叫做Streaming sidecar container），但需要注意的是，这时候，宿主机上实际上会存在两份相同的日志文件：一份是应用自己写入的；另一份则是 sidecar 的 stdout 和 stderr 对应的 JSON 文件。这对磁盘是很大的浪费 , 所以说，除非万不得已或者应用容器完全不可能被修改。
```

![img](https://pic4.zhimg.com/80/v2-ef40c9f06365b9f84e7ab096348db62f_720w.jpg)

2）

```text
 每一个pod中都起一个日志收集agent（比如logstash或fluebtd）也就是相当于把方案一里的 logging agent放在了pod里。但是这种方案资源消耗(cpu，内存)较大，并且日志不会输出到标准输出，kubectl logs 会看不到日志内容。
```

![img](https://pic2.zhimg.com/80/v2-5dfca128cae6fa73187db1bdc36b8235_720w.jpg)

### **③应用容器中直接将日志推到存储后端**

```text
这种方式就比较简单了，直接在应用里面将日志内容发送到日志收集服务后端。
```

![img](https://pic1.zhimg.com/80/v2-371db90caf8e88f1fdadb60f053ba06c_720w.jpg)

### **2、日志架构**

```text
通过上文对k8s日志收集方案的介绍，要想设计一个统一的日志收集系统，可以采用节点代理方式收集每个节点上容器的日志，日志的整体架构如图所示。
```

![img](https://pic4.zhimg.com/80/v2-4f5334b2280306753788c1047adaa25f_720w.jpg)

```text
关于ELK：
 E:elasticsearch(数据库)
 L:logstash（筛选器）
 K:kibana（展示器）
elasticsearch的运行原理：将热数据存放在内存，
```

**架构解释**

```text
所有应用容器都是基于s6基底镜像的，容器应用日志都会重定向到宿主机的某个目录文件下比如/data/logs/namespace/appname/podname/log/xxxx.log
log-agent 内部 包含 filebeat ，logrotate 等工具，其中filebeat是作为日志文件收集的agent
通过filebeat将收集的日志发送到kafka
kafka在讲日志发送的es日志存储/kibana检索层
logstash 作为中间工具主要用来在es中创建index和消费kafka 的消息
```

**解决的问题**

```text
1、用户部署的新应用，如何动态更新filebeat配置，
2、如何保证每个日志文件都被正常的rotate，
3、如果需要更多的功能则需要二次开发filebeat，使filebeat 支持更多的自定义配置。
```

## **29、k8s环境下，怎么代码上线？**

![img](https://pic2.zhimg.com/80/v2-4af4894c78b3669cde7534bc671134e1_720w.jpg)

![img](https://pic4.zhimg.com/80/v2-b6899b2a14e44ec905064e855dc446a3_720w.jpg)

## **30、kubernetes的service有哪些类型？**

### **1.ClusterIP内网**

```text
 kubernetes 默认就是这种方式，是集群内部访问的方式，外部是无法访问的。其主要用于为集群内 Pod 访 问时,提供的固定访问地址,默认是自动分配地址,可使用 ClusterIP 关键字指定固定 IP。
```

### **2.nodeport外网**

```text
 NodePort 是将主机 IP 和端口跟 kubernetes 集群所需要暴露的 IP 和端口进行关联，方便其对外提供服务。内部可以通过 ClusterIP 进行访问，外部用户可以通过 NodeIP:NodePort 的方式单独访问每个 Node 上的实例。
```

### **3.LoadBalancer弹性公网**

```text
 LoadBalancer 类型的 service 是可以实现集群外部访问服务的另外一种解决方案。不过并不是所有的 k8s集群都会支持，大多是在公有云托管集群中会支持该类型。负载均衡器是异步创建的，关于被提供的负载均衡器的信息将会通过 Service 的 status.loadBalancer 字段被发布出去。
```

### **4.ExternalName：将其他链接设置一个集群内部的别名。**

```text
 ExternalName Service 是 Service 的一个特例，它没有选择器，也没有定义任何端口或 Endpoints。它的作用是返回集群外 Service 的外部别名。它将外部地址经过集群内部的再一次封装(实际上就是集群 DNS 服务器将CNAME 解析到了外部地址上)，实现了集群内部访问即可。例如你们公司的镜像仓库，最开始是用 ip 访问，等到后面域名下来了再使用域名访问。你不可能去修改每处的引用。但是可以创建一个 ExternalName，首先指向到 ip，等后面再指向到域名。
```

### **5.headless service域名(属于ClusterIP)**

```text
 kubernates 中还有一种 service 类型：headless serivces 功能，字面意思无 service 其实就是改 service 对外无提供 IP。一般用于对外提供域名服务的时候。
 
 Service与Pod之间的关系
 service -> endprints -> pod
```

### **补充：Ingress（配合headless使用）**

```text
 Ingress为Kubernetes集群中的服务提供了入口，可以提供负载均衡、SSL终止和基于名称的虚拟主机，在生产环境中常用的Ingress有Treafik、Nginx、HAProxy、Istio等。在Kubernetesv 1.1版中添加的Ingress用于从集群外部到集群内部Service的HTTP和HTTPS路由，流量从Internet到Ingress再到Services最后到Pod上，通常情况下，Ingress部署在所有的Node节点上。Ingress可以配置提供服务外部访问的URL、负载均衡、终止SSL，并提供基于域名的虚拟主机。但Ingress不会暴露任意端口或协议。
 
 HeadLessService实际上是属于ClusterIP

 nginx ingress : 性能强
 traefik : 原生支持k8s
 istio : 服务网格，服务流量的治理
 
 service ---> endpoints  ---> pod
 ingress ---> endpoints  ---> pod
```

## **31、Pod的调度算法**

### **1.deployment全自动调度**

```text
 运行在哪个节点上完全由master的scheduler经过一系列的算法计算得出, 用户无法进行干预
```

### **2.nodeselector定向调度**

```text
1、指定pod调度到一些node上, 通过设置node的标签和deployment的nodeSelctor属性相匹配
 
2、多个node有相同标签, scheduler 会选择一个可用的node进行调度
 
3、nodeselector定义的标签匹配不上任何node上的标签， 这个pod是无法调度

4、k8s给node预定义了一些标签， 通过kubectl describe node xxxx进行查看

5、用户可以使用k8s给node预定义的标签
```

### **3.NodeAffinity： node节点亲和性**

```text
 硬限制 ： 必须满足指定的规则才可以调度pod到node上
 软限制 ： 优先满足指定规则，调度器会尝试调度pod到node上， 但不强求
```

### **4.PodAffinity：pod亲和与互斥调度**

```text
根据在节点上正在运行的pod标签而不是节点的标签进行判断和调度
 亲和： 匹配标签两个pod调度到同一个node上
 互斥： 匹配标签两个pod不能运行在同一个node上
```

## **32、如何升级k8s新版本**

### **1、配置k8s国内yum源**

```text
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

### **2、查找可更新版本**

```text
$ yum list updates | grep 'kubeadm'

## kubeadm中包含kuberctl，所以kubectl不用升级
```

### **3、升级kubeadm版本**

```text
yum install -y kubeadm*
```

### **4、查看新版本的容器镜像版本**

```text
$ kubeadm config images list
```

### **5、更换镜像下载地址**

```text
MY_REGISTRY=registry.aliyuncs.com/google_containers
```

### **6、拉取镜像**

```text
## 拉取镜像
docker pull ${MY_REGISTRY}/k8s-gcr-io-kube-apiserver:v1.20.5
docker pull ${MY_REGISTRY}/k8s-gcr-io-kube-controller-manager:v1.20.5
docker pull ${MY_REGISTRY}/k8s-gcr-io-kube-scheduler:v1.20.5
docker pull ${MY_REGISTRY}/k8s-gcr-io-kube-proxy:v1.20.5
docker pull ${MY_REGISTRY}/k8s-gcr-io-etcd:3.3.10
docker pull ${MY_REGISTRY}/k8s-gcr-io-pause:3.1
docker pull ${MY_REGISTRY}/k8s-gcr-io-coredns:1.3.6
```

### **7、打标签**

```text
## 添加Tag
docker tag ${MY_REGISTRY}/k8s-gcr-io-kube-apiserver:v1.13.4 k8s.gcr.io/kube-apiserver:v1.13.4
docker tag ${MY_REGISTRY}/k8s-gcr-io-kube-scheduler:v1.13.4 k8s.gcr.io/kube-scheduler:v1.13.4
docker tag ${MY_REGISTRY}/k8s-gcr-io-kube-controller-manager:v1.13.4 k8s.gcr.io/kube-controller-manager:v1.13.4
docker tag ${MY_REGISTRY}/k8s-gcr-io-kube-proxy:v1.13.4 k8s.gcr.io/kube-proxy:v1.13.4
docker tag ${MY_REGISTRY}/k8s-gcr-io-etcd:3.2.24 k8s.gcr.io/etcd:3.2.24
docker tag ${MY_REGISTRY}/k8s-gcr-io-pause:3.1 k8s.gcr.io/pause:3.1
docker tag ${MY_REGISTRY}/k8s-gcr-io-coredns:1.2.6 k8s.gcr.io/coredns:1.2.6
```

### **8、检测k8s新版本**

```text
kubeadm upgrade plan
```

**显示**

```text
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: 版本号
[upgrade/versions] kubeadm version: 版本号
I0316 13:06:25.721868   10166 version.go:94] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable.txt": Get https://storage.googleapis.com/kubernetes-release/release/stable.txt: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
I0316 13:06:25.721890   10166 version.go:95] falling back to the local client version: 版本号
[upgrade/versions] Latest stable version: 版本号
[upgrade/versions] Latest version in the 版本号series: 版本号
 
Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     5 x 版本号   版本号
 
Upgrade to the latest version in the v1.13 series:
 
COMPONENT            CURRENT   AVAILABLE
API Server           版本号     版本号
Controller Manager   版本号     版本号
Scheduler            版本号     版本号
Kube Proxy           版本号     版本号
CoreDNS              版本号     版本号
Etcd                 版本号     版本号
 
You can now apply the upgrade by executing the following command:
 
 kubeadm upgrade apply 版本号
```

### **9、查看各个节点状态**

```text
$ kubectl get nodes -o wide
```

### **10、升级k8s**

```text
kubeadm upgrade apply 新版本号
```

**成功显示**

```text
[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.13.4". Enjoy!
 
[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
```

### **11、升级节点逐个升级**

### **1）设置节点不可调度**

```text
$ kubectl drain 节点名称 --ignore-daemonsets
```

### **2）查找可用的kubelet升级包**

```text
$ yum list updates | grep 'kubelet'
```

### **3）升级kubelet**

```text
$ yum install -y kubelet-新版本号
```

**master节点运行时会报错**

> 在 master 节点上执行这个命令时，预计会出现下面这个错误，该错误是可以安全忽略的（因为 master 节点上有 static pod 运行）：

```text
ode "master" already cordoned
error: pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override): etcd-kubeadm, kube-apiserver-kubeadm, kube-controller-manager-kubeadm, kube-scheduler-kubeadm
```

### **4）重启**

```text
# 重新加载系统配置
$ systemctl daemon-reload
 
# 重启kubelet
$ systemctl restart kubelet
 
# 查看kubelet状态
$ systemctl status kubelet
```

### **5）设置可调度**

```text
$ kubectl uncordon 节点名称
```

### **6）查看**

> 在对集群中所有节点的 kubelet 进行升级之后，请执行以下命令，以确认所有节点又重新变为可用状态

```text
$ kubectl get nodes -o wide
```

### **12、故障恢复**

```text
如果 kubeadm upgrade 因某些原因失败并且不能回退（例如：执行过程中意外的关闭了节点实例），您可以再次运行 kubeadm upgrade，因为其具有幂等性，所以最终应该能够保证集群的实际状态就是您声明的所需状态。

您可以在使用 kubeadm upgrade 命令时带上 –force 来忽略某些启动时候的错误 参集群，使其从故障状态恢复。

kubeadm upgrade --force 
```

## **33、K8S哪些组件用到证书？**

### **1.Etcd**

```text
1、Etcd对外提供服务，要有一套etcd server证书

2、Etcd各节点之间进行通信，要有一套etcd peer证书

3、Kube-APIserver访问Etcd，要有一套etcd client证书
```

### **2.kubernetes**

```text
4、Kube-APIserver对外提供服务，要有一套kube-apiserver server证书

5、kube-scheduler、kube-controller-manager、kube-proxy、kubelet和其他可能用到的组件，需要访问kube-APIserver，要有一套kube-APIserver client证书

6、kube-controller-manager要生成服务的service account，要有一对用来签署service account的证书(CA证书)

7、kubelet对外提供服务，要有一套kubelet server证书

8、kube-APIserver需要访问kubelet，要有一套kubelet client证书
```

## **34、ingress控制器有哪些?**

```text
Kubernetes Ingress
Nginx Ingress
Kong Ingress
Traefik Ingress
HAProxy Ingress
Istio Ingress
APISIX Ingress
```

### **Kubernetes Ingress**

> ***[github.com/kubernetes/ingress-nginx](https://link.zhihu.com/?target=https%3A//github.com/kubernetes/ingress-nginx)***

Kubernetes Ingress的官方推荐的Ingress控制器，它基于nginx Web服务器，并补充了一组用于实现额外功能的Lua插件。

由于Nginx的普及使用，在将应用迁移到K8S后，该Ingress控制器是最容易上手的控制器，而且学习成本相对较低，如果你对控制器的能力要求不高，建议使用。

不过当配置文件太多的时候，Reload是很慢的，而且虽然可用插件很多，但插件扩展能力非常弱。

### **Nginx Ingress**

> ***[github.com/nginxinc/kubernetes-ingress](https://link.zhihu.com/?target=https%3A//github.com/nginxinc/kubernetes-ingress)***

Nginx Ingress是NGINX开发的官方版本，它基于NGINX Plus商业版本，NGINX控制器具有很高的稳定性，持续的向后兼容性，没有任何第三方模块，并且由于消除了Lua代码而保证了较高的速度（与官方控制器相比）。

相比官方控制器，它支持TCP/UDP的流量转发，付费版有很广泛的附加功能，主要缺点就是缺失了鉴权方式、流量调度等其他功能。

### **Kong Ingress**

> ***[github.com/Kong/kubernetes-ingress-controller](https://link.zhihu.com/?target=https%3A//github.com/Kong/kubernetes-ingress-controller)***

Kong Ingress建立在NGINX之上，并增加了扩展其功能的Lua模块。

kong在之前是专注于API网关，现在已经成为了成熟的Ingress控制器，相较于官方控制器，在路由匹配规则、upstream探针、鉴权上做了提升，并且支持大量的模块插件，并且便与配置。

它提供了一些 API、服务的定义，可以抽象成 Kubernetes 的 CRD，通过Kubernetes Ingress 配置便可完成同步状态至 Kong 集群。

### **Traefik Ingress**

> ***[github.com/containous/traefik](https://link.zhihu.com/?target=https%3A//github.com/containous/traefik)***

traefik Ingress是一个功能很全面的Ingress，官方称其为：Traefik is an Edge Router that makes publishing your services a fun and easy experience.

它具有许多有用的功能：连续更新配置（不重新启动），支持多种负载平衡算法，Web UI，指标导出，支持各种协议，REST API，Canary版本等。开箱即用的“Let's Encrypt”支持是另一个不错的功能。而且在2.0版本已经支持了TCP / SSL，金丝雀部署和流量镜像/阴影等功能，社区非常活跃。

### **Istio Ingress**

> ***[istio.io/docs/tasks/traffic-management/ingress](https://link.zhihu.com/?target=https%3A//istio.io/docs/tasks/traffic-management/ingress/)***

Istio是IBM，Google和Lyft（Envoy的原始作者）的联合项目，它是一个全面的服务网格解决方案。它不仅可以管理所有传入的外部流量（作为Ingress控制器），还可以控制集群内部的所有流量。在幕后，Istio将Envoy用作每种服务的辅助代理。从本质上讲，它是一个可以执行几乎所有操作的大型处理器。其中心思想是最大程度的控制，可扩展性，安全性和透明性。

借助Istio Ingress，您可以微调流量路由，服务之间的访问授权，平衡，监控，金丝雀发布等.

不过社区现在更推荐使用Ingress Gateways。

### **HAProxy Ingress**

> ***[github.com/jcmoraisjr/haproxy-ingress](https://link.zhihu.com/?target=https%3A//github.com/jcmoraisjr/haproxy-ingress)***

HAProxy作为王牌的负载均衡器，在众多控制器中最大的优势还在负载均衡上。

它提供了“软”配置更新（无流量丢失），基于DNS的服务发现，通过API的动态配置。HAProxy还支持完全自定义配置文件模板（通过替换ConfigMap）以及在其中使用Spring Boot函数。

### **APISIX Ingress**

> **[github.com/api7/ingress-controller](https://link.zhihu.com/?target=https%3A//github.com/api7/ingress-controller)**

ApiSix Ingress是一个新兴的Ingress Controller，它主要对标Kong Ingress。

它具有非常强大的路由能力、灵活的插件拓展能力，在性能上表现也非常优秀。同时，它的缺点也非常明显，尽管APISIX开源后有非常多的功能，但是缺少落地案例，没有相关的文档指引大家如何使用这些功能。

![img](https://pic4.zhimg.com/80/v2-17870af4b0f2686fac56d79be5d561c7_720w.jpg)

## **35、上家公司K8S用什么方式安装的？**

kubeadm，

二进制只是为了方便了解架构，一般公司不会使用

## **36、K8S运维过程中遇到了哪些坑？**

> 问这个问题主要是想试试你到底有没有工作经验

### **1、nfs数据挂载时NoRouteToHost**

**原因**

```text
防火墙开启

# 查看防火墙的状态
systemctl status firewalld.service
# 执行后，出现 绿色字体active（running），证明防火墙开启中

# 关闭防火墙（开机的时候防火墙还是会重启）
systemctl stop firewalld.service 

# 永久关闭
systemctl disable firewalld.service

# 开启
systemctl enable firewalld.service
```

### **2、某node节点master无法部署job**

```text
原因：磁盘空间满了

解决办法：1、node清理磁盘空间
   2、master重新下发任务
```

### **3、某个node节点一直NotReady**

```text
原因：
 宕机重启之后相关服务未启（未添加自启动），手动启动


systemctl restart flanneld && systemctl status flanneld
systemctl restart kube-proxy && systemctl status kube-proxy
systemctl restart kubelet && systemctl status kubelet
```

### **4、kubelet服务无法启动Failed to start Kubernetes API Server**

```text
原因：是由于系统交换内存导致，关闭即可：swapoff -a，重新启动就好了。
```

### **5、高版本docker与老版本linux内核不兼容，导致内容泄露**

### **1.问题表现**

```text
Jun  8 03:03:28 [node hostname] kernel: [88820.247535] SLUB: Unable to allocate memory on node -1 (gfp=0x8020)
Jun  8 03:03:28 [node hostname] kernel: [88820.247541]   cache: kmalloc-192(33:b40c3884668600191e02b3c32efaf12bdd1e6d0a4d3869662045e4193af6c26c), object size: 192, buffer size: 192, default order: 0, min order: 0
Jun  8 03:03:28 [node hostname] kernel: [88820.247543]   node 0: slabs: 202, objs: 4242, free: 0
Jun  8 03:03:28 [node hostname] kernel: [88820.254869] SLUB: Unable to allocate memory on node -1 (gfp=0x8020)
Jun  8 03:03:28 [node hostname] kernel: [88820.254873]   cache: kmalloc-192(33:b40c3884668600191e02b3c32efaf12bdd1e6d0a4d3869662045e4193af6c26c), object size: 192, buffer size: 192, default order: 0, min order: 0
Jun  8 03:03:28 [node hostname] kernel: [88820.254875]   node 0: slabs: 202, objs: 4242, free: 0
```

> 重启后，机器仍不断打出该log。但`free`查看内存，发现内存有空闲。

### **2.查阅资料发现内核过低**









参考：https://zhuanlan.zhihu.com/p/401179339

https://cloud.tencent.com/developer/article/1628686